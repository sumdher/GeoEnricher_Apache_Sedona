{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is encypted content. \n",
    "# Contact me: \"sudheer.siddabattula@gmail.com\" for details/access.\n",
    "-----BEGIN PGP MESSAGE-----
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=oxfF
-----END PGP MESSAGE-----
"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/21 17:59:29 WARN Utils: Your hostname, marvin resolves to a loopback address: 127.0.1.1; using 172.20.27.4 instead (on interface eth0)\n",
      "25/02/21 17:59:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/21 17:59:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/21 17:59:37 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o32.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo\norg.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m obj \u001b[38;5;241m=\u001b[39m Enricher(crs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:3035\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m, in \u001b[0;36mEnricher.setup_cluster\u001b[0;34m(self, which, ex_mem, dr_mem, log_level)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwherobots\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# config = SedonaContext.builder(). \\\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#     config(\"spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\",\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#         \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m#     getOrCreate()\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     config \u001b[38;5;241m=\u001b[39m SedonaContext\u001b[38;5;241m.\u001b[39mbuilder() \\\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.serializer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.serializer.KryoSerializer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.kryo.registrator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.sedona.core.serde.SedonaKryoRegistrator\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> 49\u001b[0m     sedona \u001b[38;5;241m=\u001b[39m \u001b[43mSedonaContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msedona \u001b[38;5;241m=\u001b[39m SedonaContext\u001b[38;5;241m.\u001b[39mcreate(config)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msedona\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mdefaultParallelism\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/sedona/spark/SedonaContext.py:43\u001b[0m, in \u001b[0;36mSedonaContext.create\u001b[0;34m(cls, spark)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mcls\u001b[39m, spark: SparkSession) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SparkSession:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    This is the core of whole package, It uses py4j to run wrapper which takes existing SparkSession\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    and register the core logics of Apache Sedona, for this SparkSession.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    :return: SedonaContext which is an instance of SparkSession\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT 1 as geom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# with Spark Connect there is no local JVM\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote():\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo\norg.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "obj = Enricher(crs=\"EPSG:3035\")\n",
    "obj.setup_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/21 17:55:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/02/21 17:55:41 ERROR TorrentBroadcast: Store broadcast broadcast_1 fail, remove all pieces of the broadcast\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo\norg.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     19\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountries\u001b[39m\u001b[38;5;124m\"\u001b[39m: (path_contr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapefile\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregions_IT\u001b[39m\u001b[38;5;124m\"\u001b[39m: (path_reg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapefile\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# \"census\": (path_census, \"\"),\u001b[39;00m\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     31\u001b[0m obj \u001b[38;5;241m=\u001b[39m Enricher(crs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:3035\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msedona\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m26\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdr_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mERROR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m obj\u001b[38;5;241m.\u001b[39mload(datasets, data_dir\u001b[38;5;241m=\u001b[39mdata_dir, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m obj\u001b[38;5;241m.\u001b[39mfix_geometries(skip\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpop_grids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpop_grids_new\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m, in \u001b[0;36mEnricher.setup_cluster\u001b[0;34m(self, which, ex_mem, dr_mem, log_level)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m which \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msedona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# config = SedonaContext.builder() .\\\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m#     config(\"spark.executor.memory\", f\"{ex_mem}g\").\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m#         'org.datasyslab:geotools-wrapper:1.7.0-28.5').\\\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#     getOrCreate()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     config \u001b[38;5;241m=\u001b[39m SedonaContext\u001b[38;5;241m.\u001b[39mbuilder() \\\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex_mem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr_mem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg.datasyslab:geotools-wrapper:1.7.0-28.5\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msedona \u001b[38;5;241m=\u001b[39m \u001b[43mSedonaContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_level \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOFF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msedona\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(log_level)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/sedona/spark/SedonaContext.py:43\u001b[0m, in \u001b[0;36mSedonaContext.create\u001b[0;34m(cls, spark)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mcls\u001b[39m, spark: SparkSession) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SparkSession:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    This is the core of whole package, It uses py4j to run wrapper which takes existing SparkSession\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    and register the core logics of Apache Sedona, for this SparkSession.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    :return: SedonaContext which is an instance of SparkSession\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT 1 as geom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# with Spark Connect there is no local JVM\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote():\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo\norg.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# file paths:\n",
    "\n",
    "path_contr = f\"{data_dir}/data_EU/countries_shp/\"\n",
    "path_reg = f\"{data_dir}/data_Italy/regioni/\"\n",
    "path_prov = f\"{data_dir}/data_Italy/provinci\"\n",
    "path_com_EU = f\"{data_dir}/data_EU/comuni_shp/\"\n",
    "path_com = f\"{data_dir}/data_Italy/comuni/\"\n",
    "path_grids = f\"{data_dir}/data_EU/census_grid_EU/grids_corrected.parquet\"\n",
    "path_grids_new = f\"{data_dir}/data_EU/census_grid_EU/grids_new.gpkg\"\n",
    "\n",
    "\n",
    "# datasets:\n",
    "# format: {display_name: (path, file_format), ...}\n",
    "\n",
    "datasets = {\n",
    "    \"countries\": (path_contr, \"shapefile\"),\n",
    "    \"regions_IT\": (path_reg, \"shapefile\"),\n",
    "    \"provinces_IT\": (path_prov, \"shapefile\"),\n",
    "    \"comuni_EU\": (path_com_EU, \"shapefile\"),\n",
    "    \"comuni_IT\": (path_com, \"shapefile\"),\n",
    "    \"pop_grids\": (path_grids, \"geoparquet\"),\n",
    "    \"pop_grids_new\": (path_grids_new, \"geopackage\")\n",
    "    # \"census\": (path_census, \"\"),\n",
    "}\n",
    "\n",
    "\n",
    "obj = Enricher(crs=\"EPSG:3035\")\n",
    "obj.setup_cluster(which=\"sedona\", ex_mem=26, dr_mem=24, log_level=\"ERROR\")\n",
    "\n",
    "obj.load(datasets, data_dir=data_dir, silent=True)\n",
    "obj.fix_geometries(skip=['pop_grids', 'pop_grids_new'])\n",
    "obj.force_repartition(skip=['pop_grids'])\n",
    "# obj.inspect_partitions()\n",
    "obj.transform_CRS(lazy=False)\n",
    "obj.parquet_all(preserve_partitions=True)\n",
    "\n",
    "\n",
    "# obj.dfs_list['comuni_EU'] = obj.dfs_list['comuni_EU'].filter(F.col('CNTR_ID').isin([\"IT\", \"DE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'countries' from Parquet...\n",
      "Loaded dataframe 'countries'\n",
      "Loading 'regions_IT' from Parquet...\n",
      "Loaded dataframe 'regions_IT'\n",
      "Loading 'provinces_IT' from Parquet...\n",
      "Loaded dataframe 'provinces_IT'\n",
      "Loading 'comuni_EU' from Parquet...\n",
      "Loaded dataframe 'comuni_EU'\n",
      "Loading 'comuni_IT' from Parquet...\n",
      "Loaded dataframe 'comuni_IT'\n",
      "Loading 'pop_grids' from Parquet...\n",
      "Loaded dataframe 'pop_grids'\n",
      "Loading 'pop_grids_new' from Parquet...\n",
      "Loaded dataframe 'pop_grids_new'\n"
     ]
    }
   ],
   "source": [
    "# Load from pickles\n",
    "\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# file paths:\n",
    "\n",
    "path_contr = f\"{data_dir}/data_EU/countries_shp/\"\n",
    "path_reg = f\"{data_dir}/data_Italy/regioni/\"\n",
    "path_prov = f\"{data_dir}/data_Italy/provinci\"\n",
    "path_com_EU = f\"{data_dir}/data_EU/comuni_shp/\"\n",
    "path_com = f\"{data_dir}/data_Italy/comuni/\"\n",
    "path_grids = f\"{data_dir}/data_EU/census_grid_EU/grids_corrected.parquet\"\n",
    "path_grids_new = f\"{data_dir}/data_EU/census_grid_EU/grids_new.gpkg\"\n",
    "\n",
    "\n",
    "# datasets:\n",
    "# format: {display_name: (path, file_format), ...}\n",
    "\n",
    "datasets = {\n",
    "    \"countries\": (path_contr, \"shapefile\"),\n",
    "    \"regions_IT\": (path_reg, \"shapefile\"),\n",
    "    \"provinces_IT\": (path_prov, \"shapefile\"),\n",
    "    \"comuni_EU\": (path_com_EU, \"shapefile\"),\n",
    "    \"comuni_IT\": (path_com, \"shapefile\"),\n",
    "    \"pop_grids\": (path_grids, \"geoparquet\"),\n",
    "    \"pop_grids_new\": (path_grids_new, \"geopackage\")\n",
    "    # \"census\": (path_census, \"\"),\n",
    "}\n",
    "\n",
    "# obj = Enricher(crs=\"EPSG:3035\")\n",
    "# obj.setup_cluster(which=\"sedona\", ex_mem=26, dr_mem=24, log_level=\"ERROR\")\n",
    "obj.load_from_parquets(datasets)\n",
    "# obj.inspect_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMM_ID', 'CNTR_ID', 'CNTR_CODE', 'COMM_NAME', 'NAME_ASCI', 'TRUE_FLAG', 'NSI_CODE', 'NAME_NSI', 'NAME_LATN', 'NUTS_CODE', 'FID', 'DIST_BORD', 'TOT_P_2018', 'TOT_P_2006', 'GRD_ID', 'TOT_P_2011', 'Y_LLC', 'NUTS2016_3', 'NUTS2016_2', 'NUTS2016_1', 'NUTS2016_0', 'LAND_PC', 'X_LLC', 'NUTS2021_3', 'NUTS2021_2', 'NUTS2021_1', 'NUTS2021_0', 'geometry', 'intr_ratio', 'TOT_P_2021_agr_sum', 'DIST_COAST_agr_mean']\n",
      "Saving dataframe 'pop_new_X_comuni_EU_res' as Parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to './pickle_parquets/others/pop_new_X_comuni_EU_res'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# obj.parquet_this(\"pop_new_X_comuni_EU\", obj.res_agr, preserve_partitions=False)\n",
    "\n",
    "obj.parquet_this(\"pop_new_X_comuni_EU_res\", obj.res, preserve_partitions=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20138849a0a4028a69f7f4d345753f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Enrich with Overlay & Aggregation</h1>'), HTML(value=\"<div style='height: 5px;'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GUI\n",
    "\n",
    "obj_ui = EnricherUI(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unique_values = obj.res.select('CNTR_ID').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m shape\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdecimal\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241m.\u001b[39mres_agr\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# temp_df = obj.res.filter(F.col('CNTR_ID').isin(\"NL\", \"BE\", \"DE\", \"IT\"))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# temp_df = obj.dfs_list['comuni_EU']\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# temp_df = obj.dfs_list['pop_grids'].filter(F.col('T')>1000)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprep_for_map\u001b[39m(res_agr, crs, geom_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'obj' is not defined"
     ]
    }
   ],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import decimal\n",
    "\n",
    "temp_df = obj.res_agr\n",
    "# temp_df = obj.res.filter(F.col('CNTR_ID').isin(\"NL\", \"BE\", \"DE\", \"IT\"))\n",
    "# temp_df = obj.dfs_list['comuni_EU']\n",
    "# temp_df = obj.dfs_list['pop_grids'].filter(F.col('T')>1000)\n",
    "\n",
    "\n",
    "def prep_for_map(res_agr, crs, geom_col='geometry'):\n",
    "    df = res_agr.toPandas()\n",
    "    df = df.map(lambda x: float(x) if isinstance(x, decimal.Decimal) else x)\n",
    "    df['geometry'] = df[f'{geom_col}'].apply(lambda geom: shape(geom))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    gdf.crs = crs\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=prep_for_map(temp_df, obj.crs, geom_col=\"geometry\"), name=\"pop_grids\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import decimal\n",
    "\n",
    "# temp_df = obj.res_agr\n",
    "temp_df = obj.dfs_list['pop_grids_new']\n",
    "# temp_df = obj.dfs_list['comuni_EU']\n",
    "# temp_df = obj.dfs_list['pop_grids_new'].filter(F.col('CNTR_ID').isin(\"NL\", \"BE\", \"DE\", \"IT\"))\n",
    "\n",
    "\n",
    "def prep_for_map(res_agr, crs, geom_col='geometry'):\n",
    "    df = res_agr.toPandas()\n",
    "    df = df.map(lambda x: float(x) if isinstance(x, decimal.Decimal) else x)\n",
    "    df['geometry'] = df[f'{geom_col}'].apply(lambda geom: shape(geom))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    gdf.crs = crs\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=prep_for_map(temp_df, obj.crs, geom_col=\"geometry\"), name=\"pop_grids\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_EU_com_enriched = obj.res_agr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a537a4fb6604e8fab471ca6fb390dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'res_agr': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "com_pop = obj.res_agr\n",
    "\n",
    "res_agr = com_pop.toPandas()\n",
    "res_agr['geometry'] = res_agr['geometry'].apply(lambda geom: shape(geom))\n",
    "\n",
    "gdf = gpd.GeoDataFrame(res_agr, geometry='geometry')\n",
    "gdf.crs = \"EPSG:3035\"\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=gdf, name=\"res_agr\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedona_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
