{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b21a328-237b-430e-8dce-576ad78fbe4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/14 15:42:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "sc.defaultParallelism: 10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"~Wherobots setup\"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-test').getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "num_partitions = sc.defaultParallelism\n",
    "print(f\"sc.defaultParallelism: {num_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4401d-e0f1-4c38-92de-6205cf2ba34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635defc2-782c-4c0e-873d-9e702bca4004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# from sedona.spark import *\n",
    "# from sedona.spark import SedonaContext, Adapter\n",
    "# from contextlib import contextmanager\n",
    "# import time\n",
    "# from pyspark.sql.functions import col, expr\n",
    "# from pyspark.sql.functions import desc\n",
    "# from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "# config = SedonaContext.builder().appName('sedona-test').getOrCreate()\n",
    "\n",
    "# sedona = SedonaContext.create(config)\n",
    "# sc = sedona.sparkContext\n",
    "# print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "# print(f\"sc.defaultParallelism: {sc.defaultParallelism}\")\n",
    "\n",
    "# \"\"\"\"\"\"\n",
    "# num_partitions = sc.defaultParallelism\n",
    "# print(f\"using partitions: {num_partitions}\")\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType\n",
    "# from geospark.utils.adapter import Adapter\n",
    "\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "    \n",
    "def chudu(df):\n",
    "    \n",
    "    df = df.toPandas()\n",
    "    rows, cols = df.shape\n",
    "    print(f\"Total rows: {rows}, Total columns: {cols}\")\n",
    "    \n",
    "    memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage: {memory:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head(5))\n",
    "    \n",
    "    unique_fields = [col for col in df.columns if df[col].is_unique]\n",
    "    print(\"\\nCols with unique values no repeats:\")\n",
    "    print(unique_fields if unique_fields else \"None\")\n",
    "    \n",
    "    nan_columns = df.isna().sum()\n",
    "    nan_columns = nan_columns[nan_columns > 0]\n",
    "    print(\"\\nColumns with NaN:\")\n",
    "    print(nan_columns if not nan_columns.empty else \"None\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "# countries_path = \"./my_data/data_EU/countries_shp/\"\n",
    "# countries_rdd = ShapefileReader.readToGeometryRDD(sc, countries_path)\n",
    "\n",
    "italy_rdd = ShapefileReader.readToGeometryRDD(sc, \"./my_data/data_Italy/merged/merged_pop_geom/\")\n",
    "italy_rdd.analyze()\n",
    "italy_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "italy_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "italy_df = Adapter.toDf(italy_rdd, sedona)\n",
    "\n",
    "lomb_rdd = ShapefileReader.readToGeometryRDD(sc, \"./my_data/data_Italy/lomb/\")\n",
    "lomb_rdd.analyze()\n",
    "lomb_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "lomb_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "lomb_df = Adapter.toDf(lomb_rdd, sedona)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# grids_df = sedona.read.parquet(\"./my_data/data_EU/census_grid_EU/grids.parquet\")\n",
    "# grids_df = grids_df.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n",
    "# grids_df = grids_df.withColumn(\n",
    "#     \"GRD_ID\", regexp_replace(\"GRD_ID\", \"CRS3035RES1000m\", \"\")\n",
    "# )\n",
    "# grids_df = grids_df.select(\"GRD_ID\", \"T\", \"geom\")\n",
    "# grids_rdd = Adapter.toSpatialRdd(grids_df)\n",
    "# grids_rdd.analyze()\n",
    "# grids_rdd.boundaryEnvelope\n",
    "\n",
    "# countries_rdd.analyze()\n",
    "\n",
    "# countries_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "# countries_rdd.spatialPartitioning(com_rdd.getPartitioner())\n",
    "# grids_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "# countries_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "\n",
    "# countries_df = Adapter.toDf(countries_rdd, sedona)\n",
    "# countries_df = countries_df.select(\"CNTR_ID\", \"NAME_ENGL\", \"geometry\")\n",
    "\n",
    "\n",
    "# grids_df = grids_df.withColumn(\"geom\", expr(\"ST_AsText(geom)\"))\n",
    "\n",
    "italy_df = italy_df.repartition(num_partitions * 2)\n",
    "lomb_df = lomb_df.repartition(num_partitions * 2)\n",
    "\n",
    "joined_df = italy_df.alias(\"it\").join(\n",
    "    lomb_df.alias(\"itt\"),\n",
    "    expr(\"ST_Intersects(it.geometry, itt.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "# chudu(joined_df)\n",
    "\n",
    "# print(joined_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b43286-02a9-4067-8a6b-4f248405e5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/14 14:47:09 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1477)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "25/01/14 14:47:23 ERROR Executor: Exception in task 21.0 in stage 27.0 (TID 119)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.sedona.common.geometrySerde.GeometrySerde.readGeometry(GeometrySerde.java:138)\n",
      "\tat org.apache.sedona.common.geometrySerde.GeometrySerde.read(GeometrySerde.java:97)\n",
      "\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)\n",
      "\tat org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:311)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.buildIndex(DynamicIndexLookupJudgement.java:107)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.call(DynamicIndexLookupJudgement.java:83)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.call(DynamicIndexLookupJudgement.java:37)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$zipPartitions$1(JavaRDDLike.scala:320)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$Lambda$3509/0x00000008020c63c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "25/01/14 14:47:23 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#276,Executor task launch worker for task 21.0 in stage 27.0 (TID 119),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.sedona.common.geometrySerde.GeometrySerde.readGeometry(GeometrySerde.java:138)\n",
      "\tat org.apache.sedona.common.geometrySerde.GeometrySerde.read(GeometrySerde.java:97)\n",
      "\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)\n",
      "\tat org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:311)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.buildIndex(DynamicIndexLookupJudgement.java:107)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.call(DynamicIndexLookupJudgement.java:83)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.call(DynamicIndexLookupJudgement.java:37)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$zipPartitions$1(JavaRDDLike.scala:320)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$Lambda$3509/0x00000008020c63c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "25/01/14 14:47:23 WARN TaskSetManager: Lost task 21.0 in stage 27.0 (TID 119) (15de5320a420 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.sedona.common.geometrySerde.GeometrySerde.readGeometry(GeometrySerde.java:138)\n",
      "\tat org.apache.sedona.common.geometrySerde.GeometrySerde.read(GeometrySerde.java:97)\n",
      "\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)\n",
      "\tat org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:311)\n",
      "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:158)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:188)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:185)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:32)\n",
      "\tat org.apache.sedona.core.spatialRDD.SpatialRDD$2$1.hasNext(SpatialRDD.java:311)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.buildIndex(DynamicIndexLookupJudgement.java:107)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.call(DynamicIndexLookupJudgement.java:83)\n",
      "\tat org.apache.sedona.core.joinJudgement.DynamicIndexLookupJudgement.call(DynamicIndexLookupJudgement.java:37)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$zipPartitions$1(JavaRDDLike.scala:320)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$$Lambda$3509/0x00000008020c63c8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\n",
      "25/01/14 14:47:23 ERROR TaskSetManager: Task 21 in stage 27.0 failed 1 times; aborting job\n",
      "25/01/14 14:47:23 WARN TaskSetManager: Lost task 27.0 in stage 27.0 (TID 125) (15de5320a420 executor driver): TaskKilled (Stage cancelled)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_410/2975937667.py\", line 10, in <module>\n",
      "    joined_df.count()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\", line 1193, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m joined_df \u001b[38;5;241m=\u001b[39m italy_df\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      5\u001b[0m     lomb_df\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     expr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mST_Intersects(it.geometry, itt.geometry)\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mjoined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "italy_df = italy_df.repartition(num_partitions * 2)\n",
    "lomb_df = lomb_df.repartition(num_partitions * 2)\n",
    "\n",
    "joined_df = italy_df.alias(\"it\").join(\n",
    "    lomb_df.alias(\"itt\"),\n",
    "    expr(\"ST_Intersects(it.geometry, itt.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "joined_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4a964d-4e4a-4df0-a3b2-c6cf1c4eaf6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mitaly_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "italy_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47942363-96ac-4cb9-baa5-0a484dd0532e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import regexp_replace\n",
    "# from sedona.sql.types import GeometryType\n",
    "\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from sedona.sql.types import GeometryType\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-test').getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"sc.defaultParallelism: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "num_partitions = sc.defaultParallelism\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "    \n",
    "def chudu(df):\n",
    "    \n",
    "    df = df.toPandas()\n",
    "    rows, cols = df.shape\n",
    "    print(f\"Total rows: {rows}, Total columns: {cols}\")\n",
    "    \n",
    "    memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage: {memory:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head(5))\n",
    "    \n",
    "    unique_fields = [col for col in df.columns if df[col].is_unique]\n",
    "    print(\"\\nCols with unique values no repeats:\")\n",
    "    print(unique_fields if unique_fields else \"None\")\n",
    "    \n",
    "    nan_columns = df.isna().sum()\n",
    "    nan_columns = nan_columns[nan_columns > 0]\n",
    "    print(\"\\nColumns with NaN:\")\n",
    "    print(nan_columns if not nan_columns.empty else \"None\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "countries_path = \"./my_data/data_EU/countries_shp/\"\n",
    "countries_rdd = ShapefileReader.readToGeometryRDD(sc, countries_path)\n",
    "countries_df = Adapter.toDf(countries_rdd, sedona)\n",
    "\n",
    "grids_df = sedona.read.parquet(\"./my_data/data_EU/census_grid_EU/grids.parquet\")\n",
    "# grids_df = grids_df.withColumn(\"geom\", grids_df[\"geom\"].cast(GeometryType))\n",
    "grids_df = grids_df.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n",
    "\n",
    "# grids_df = grids_df.withColumn(\"geom\", expr(\"ST_AsText(geom)\"))\n",
    "\n",
    "grids_df = grids_df.withColumn(\n",
    "    \"GRD_ID\", regexp_replace(\"GRD_ID\", \"CRS3035RES1000m\", \"\")\n",
    ")\n",
    "grids_df = grids_df.select(\"GRD_ID\", \"T\", \"geom\")\n",
    "countries_df = countries_df.select(\"CNTR_ID\", \"NAME_ENGL\", \"geometry\")\n",
    "\n",
    "grids_df = grids_df.repartition(16)\n",
    "\n",
    "joined_df = grids_df.alias(\"grd\").join(\n",
    "    countries_df.alias(\"con\"),\n",
    "    expr(\"ST_Intersects(grd.geom, con.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "# chudu(joined_df)\n",
    "\n",
    "print(joined_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0241cf-703d-47cb-aca5-f7fa7d03ecca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grids_df.printSchema()\n",
    "countries_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffb5465d-60b7-44c9-9525-b599f7ff0a36",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c4c915-00ac-4078-99ca-ef0f657e02d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/07 11:28:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "def par: 8\n",
      "using partitions: 16\n",
      "\n",
      "DF JOIN\n",
      "\n",
      "Reading cens shp file... DONE in 0.52 sec\n",
      "Reading com shp file... DONE in 0.31 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Partitioning... DONE in 5.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 11:28:51 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toDf + rename cols + Spatial Join... DONE in 50.42 sec\n",
      "Job done in 56.93 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" SPATIAL PARTITIOING. DF JOIN \"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "import time\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-example-python')\\\n",
    "    .config('spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider','org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"def par: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "# num_partitions = sc.defaultParallelism * 2\n",
    "num_partitions = 16\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "print()\n",
    "print(\"DF JOIN\")\n",
    "print()\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "\n",
    "with get_time(\"Reading cens shp file\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "with get_time(\"Reading com shp file\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "    \n",
    "\n",
    "with get_time(\"Spatial Partitioning\"):\n",
    "    cens_rdd.analyze()\n",
    "    com_rdd.analyze()\n",
    "\n",
    "    cens_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "    com_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "    \n",
    "cens_df = Adapter.toDf(cens_rdd, sedona)\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns])\n",
    "\n",
    "joined_df = (\n",
    "    com_df_renamed.select(\"COMUNE_com\", \"geometry_com\")\n",
    "    .alias(\"com\")\n",
    "    .join(\n",
    "        cens_df.alias(\"cens\"),\n",
    "        expr(\"ST_Contains(com.geometry_com, cens.geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with get_time(\"toDf + rename cols + Spatial Join\"):\n",
    "    joined_df.write.format('geoparquet').mode('overwrite').save('./res_geoparquet')\n",
    "    # print(f\"Total features after join: {joined_df.count()}\")\n",
    "\n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e14f8e-c824-48b6-a980-8b338032890c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb10d04-aa36-430b-8f59-ab63dfee0ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd534e4a-382f-4d66-9946-a17a7e05aaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88025cf7-fb68-4e0e-881b-58df54a38bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/07 11:42:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "def par: 8\n",
      "using partitions: 8\n",
      "\n",
      "DF JOIN\n",
      "\n",
      "Reading cens shp file... DONE in 0.47 sec\n",
      "Reading com shp file... DONE in 0.29 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 11:42:47 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toDf + rename cols + Spatial Join... DONE in 29.21 sec\n",
      "Job done in 29.98 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" repartition(8, \"column\") DF JOIN \"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "import time\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-example-python')\\\n",
    "    .config('spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider','org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"def par: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "# num_partitions = sc.defaultParallelism * 2\n",
    "num_partitions = 8\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "print()\n",
    "print(\"DF JOIN\")\n",
    "print()\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "\n",
    "with get_time(\"Reading cens shp file\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "with get_time(\"Reading com shp file\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "    \n",
    "cens_df = Adapter.toDf(cens_rdd, sedona).repartition(num_partitions, \"COD_UTS\")\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns]).repartition(num_partitions, \"COD_UTS_com\")\n",
    "\n",
    "joined_df = (\n",
    "    com_df_renamed.select(\"COMUNE_com\", \"geometry_com\")\n",
    "    .alias(\"com\")\n",
    "    .join(\n",
    "        cens_df.alias(\"cens\"),\n",
    "        expr(\"ST_Contains(com.geometry_com, cens.geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with get_time(\"toDf + rename cols + Spatial Join\"):\n",
    "    joined_df.write.format('geoparquet').mode('overwrite').save('./res_geoparquet')\n",
    "    # print(f\"Total features after join: {joined_df.count()}\")\n",
    "\n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7084bce1-9f4d-4349-82bb-6733492d2864",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------+-------+-------+------------+----------+--------+----------+-------+--------+---------+---------+----------+----------+--------+--------+--------+-----+-----+-----------------+-----------------+\n",
      "|            geometry|COD_REG|COD_UTS|PRO_COM|  SEZ21|    SEZ21_ID|COD_TIPO_S|TIPO_LOC|  LOC21_ID|COD_ZIC|COD_ISAM|COD_ACQUE|COD_ISOLE|COD_MONT_D|COD_AREA_S|COM_ASC1|COM_ASC2|COM_ASC3|POP21|FAM21|       SHAPE_Leng|       SHAPE_Area|\n",
      "+--------------------+-------+-------+-------+-------+------------+----------+--------+----------+-------+--------+---------+---------+----------+----------+--------+--------+--------+-----+-----+-----------------+-----------------+\n",
      "|POLYGON ((516786....|      3|    215|  15146|   1528|151460001528|         1|       1|1514610006|      0|       0|        0|        0|         0|         0|15146003|15146021|       0|  197|  120|4.58618956384e+02|8.24104927375e+03|\n",
      "|POLYGON ((539453....|      3|     98|  98031|     13|980310000013|         1|       1|9803110002|      0|       0|        0|        0|         0|         0|       0|       0|       0|   99|   41|5.70214226132e+02|1.14522615799e+04|\n",
      "|POLYGON ((592861....|      3|     17|  17065|     71|170650000071|        15|       1|1706510002|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|2.48113235065e+02|3.81125417606e+03|\n",
      "|POLYGON ((531285....|      3|     97|  97058|8888888|970588888888|       100|       1|9705810001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|4.73694009160e+01|1.32729400947e+02|\n",
      "|POLYGON ((540136....|      3|    215|  15059|     98|150590000098|        16|       1|1505910002|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|3.74352845116e+02|8.23159813350e+03|\n",
      "|POLYGON ((518321....|      3|    215|  15146|   1825|151460001825|         1|       1|1514610006|      0|       0|        0|        0|         0|         0|15146003|15146018|       0|  208|  124|5.27667123324e+02|8.73566070054e+03|\n",
      "|POLYGON ((640513....|      3|     20|  20030|    623|200300000623|         5|       1|2003010005|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|2.33580457678e+02|3.43598653150e+03|\n",
      "|POLYGON ((494640....|      3|    215|  15085|     68|150850000068|         1|       1|1508510001|      0|       0|        0|        0|         0|         0|       0|       0|       0|  273|  108|1.09836466730e+03|3.18341670015e+04|\n",
      "|POLYGON ((499253....|      3|     18|  18182|    336|181820000336|        12|       1|1818210004|      0|       0|        0|        0|         0|         0|       0|       0|       0|    3|    1|1.23526080623e+03|6.31057028918e+04|\n",
      "|POLYGON ((551390....|      3|     98|  98002|      8|980020000008|         1|       2|9800226604|      0|       0|        0|        0|         0|         0|       0|       0|       0|   38|   13|9.97191253953e+02|3.03137670449e+04|\n",
      "|POLYGON ((512953....|      3|    215|  15146|   6348|151460006348|        36|       1|1514610006|      0|       0|        0|        0|         0|         0|15146009|15146077|       0|    0|    0|1.97039441275e+03|1.51370234268e+04|\n",
      "|POLYGON ((548192....|      3|     16|  16053|    156|160530000156|        36|       4|1605340001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|2.87260727013e+03|1.81640689305e+04|\n",
      "|POLYGON ((502422....|      3|     13|  13036|     49|130360000049|        36|       4|1303640001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|1.85763464534e+03|3.15170147399e+04|\n",
      "|POLYGON ((590284....|      3|     17|  17072|     13|170720000013|        12|       1|1707210002|      0|       0|        0|        0|         0|         0|       0|       0|       0|  117|   43|4.99574936024e+03|6.14598210415e+05|\n",
      "|POLYGON ((502305....|      3|     18|  18091|     11|180910000011|        29|       1|1809110001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|1.24943930979e+02|9.28696910963e+02|\n",
      "|POLYGON ((513430....|      3|    215|  15146|   5814|151460005814|         1|       1|1514610006|      0|       0|        0|        0|         0|         0|15146009|15146083|       0|  319|  144|5.83467101560e+02|1.51437382926e+04|\n",
      "|POLYGON ((488605....|      3|     18|  18177|     15|181770000015|         1|       1|1817710003|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|2.59655638304e+02|2.70614338613e+03|\n",
      "|POLYGON ((507021....|      3|     18|  18126|     27|181260000027|        15|       4|1812640001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|3.48805919358e+02|5.05267147528e+03|\n",
      "|POLYGON ((504838....|      3|    215|  15211|    180|152110000180|         1|       4|1521140001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    1|    1|2.97883895693e+02|5.40174826644e+03|\n",
      "|POLYGON ((528045....|      3|     14|  14058|     20|140580000020|         1|       2|1405824912|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|7.95160576592e+02|2.24681827157e+04|\n",
      "+--------------------+-------+-------+-------+-------+------------+----------+--------+----------+-------+--------+---------+---------+----------+----------+--------+--------+--------+-----+-----+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cens_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cebd556-e6d4-4125-b18f-a35a602e6a54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------+--------+------+-------+-------+---------+-------------------+--------+------+------------------+------------------+\n",
      "|            geometry|COD_RIP|COD_REG|COD_PROV|COD_CM|COD_UTS|PRO_COM|PRO_COM_T|             COMUNE|COMUNE_A|CC_UTS|        Shape_Leng|        Shape_Area|\n",
      "+--------------------+-------+-------+--------+------+-------+-------+---------+-------------------+--------+------+------------------+------------------+\n",
      "|POLYGON ((404703....|      1|      1|       1|     0|    201|   1001|   001001|             AgliÃ¨|       0|     0|1.80352540007e+004|1.31462576661e+007|\n",
      "|POLYGON ((380700....|      1|      1|       1|     0|    201|   1002|   001002|            Airasca|       0|     0|1.84089069884e+004|1.57393133213e+007|\n",
      "|POLYGON ((364710....|      1|      1|       1|     0|    201|   1003|   001003|       Ala di Stura|       0|     0|3.18341560810e+004|4.63315602074e+007|\n",
      "|POLYGON ((415942....|      1|      1|       1|     0|    201|   1004|   001004|    Albiano d'Ivrea|       0|     0|1.89272628270e+004|1.17396902250e+007|\n",
      "|POLYGON ((376934....|      1|      1|       1|     0|    201|   1006|   001006|             Almese|       0|     0|1.70574139642e+004|1.78741245214e+007|\n",
      "|POLYGON ((388890....|      1|      1|       1|     0|    201|   1007|   001007|            Alpette|       0|     0|9.74254551158e+003|5.63111028264e+006|\n",
      "|POLYGON ((382213....|      1|      1|       1|     0|    201|   1008|   001008|          Alpignano|       0|     0|1.79831548922e+004|1.19192532455e+007|\n",
      "|POLYGON ((410098....|      1|      1|       1|     0|    201|   1009|   001009|           Andezeno|       0|     0|1.30913340003e+004|7.48592427309e+006|\n",
      "|POLYGON ((413793....|      1|      1|       1|     0|    201|   1010|   001010|            Andrate|       0|     0|2.01374964019e+004|9.30899805827e+006|\n",
      "|POLYGON ((353807....|      1|      1|       1|     0|    201|   1011|   001011|           Angrogna|       0|     0|3.54417027386e+004|3.88781376267e+007|\n",
      "|POLYGON ((413545....|      1|      1|       1|     0|    201|   1012|   001012|           Arignano|       0|     0|1.40963290967e+004|8.16656007288e+006|\n",
      "|POLYGON ((373944....|      1|      1|       1|     0|    201|   1013|   001013|          Avigliana|       0|     0|2.58744703804e+004|2.32172747995e+007|\n",
      "|MULTIPOLYGON (((4...|      1|      1|       1|     0|    201|   1014|   001014|            Azeglio|       0|     0|2.25490384802e+004|9.95282100890e+006|\n",
      "|POLYGON ((402490....|      1|      1|       1|     0|    201|   1015|   001015|              Bairo|       0|     0|1.28614749609e+004|7.08593261854e+006|\n",
      "|POLYGON ((382311....|      1|      1|       1|     0|    201|   1016|   001016|          Balangero|       0|     0|1.84385104334e+004|1.30065097374e+007|\n",
      "|POLYGON ((402346....|      1|      1|       1|     0|    201|   1017|   001017|Baldissero Canavese|       0|     0|1.06787807036e+004|4.50640451054e+006|\n",
      "|POLYGON ((404596....|      1|      1|       1|     0|    201|   1018|   001018|Baldissero Torinese|       0|     0|2.20832482549e+004|1.54045666958e+007|\n",
      "|POLYGON ((356578....|      1|      1|       1|     0|    201|   1019|   001019|              Balme|       0|     0|3.71599832512e+004|6.27104944878e+007|\n",
      "|MULTIPOLYGON (((4...|      1|      1|       1|     0|    201|   1020|   001020|          Banchette|       0|     0|1.38601654299e+004|2.02889414066e+006|\n",
      "|POLYGON ((392550....|      1|      1|       1|     0|    201|   1021|   001021|           Barbania|       0|     0|1.64824605468e+004|1.28027373603e+007|\n",
      "+--------------------+-------+-------+--------+------+-------+-------+---------+-------------------+--------+------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "com_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3f4c32-a5a0-4b58-9a3b-6fea13b1a2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions in joined_df: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 15:47:35 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Partitions in joined_df: {joined_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0c584f-79be-46c4-abc3-6db31ccfe5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/05 16:12:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "def par: 8\n",
      "using partitions: 16\n",
      "\n",
      "DF JOIN\n",
      "\n",
      "Reading cens shp file... DONE in 0.48 sec\n",
      "Reading com shp file... DONE in 0.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 16:12:16 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toDf + rename cols + Spatial Join... DONE in 52.30 sec\n",
      "Job done in 53.02 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" DF JOIN \"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "import time\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-example-python')\\\n",
    "    .config('spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider','org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"def par: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "num_partitions = sc.defaultParallelism * 2\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "print()\n",
    "print(\"DF JOIN\")\n",
    "print()\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "\n",
    "with get_time(\"Reading cens shp file\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "with get_time(\"Reading com shp file\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "    \n",
    "cens_df = Adapter.toDf(cens_rdd, sedona)\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns])\n",
    "\n",
    "joined_df = (\n",
    "    com_df_renamed.select(\"COMUNE_com\", \"geometry_com\")\n",
    "    .alias(\"com\")\n",
    "    .join(\n",
    "        cens_df.alias(\"cens\"),\n",
    "        expr(\"ST_Contains(com.geometry_com, cens.geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with get_time(\"toDf + rename cols + Spatial Join\"):\n",
    "    joined_df.write.format('geoparquet').mode('overwrite').save('./res_geoparquet')\n",
    "    # print(f\"Total features after join: {joined_df.count()}\")\n",
    "\n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c377d5-699f-4f29-9b9a-4dbcc569ab08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/05 14:47:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "def par: 8\n",
      "using partitions: 16\n",
      "\n",
      "SQL JOIN\n",
      "\n",
      "Reading cens shp file... DONE in 0.37 sec\n",
      "Reading com shp file... DONE in 0.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 14:48:01 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 17:=====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features after join: 99886\n",
      "toDf + rename cols + SQL views + Spatial Join... DONE in 13.98 sec\n",
      "Job done in 14.54 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" SQL JOIN \"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import desc\n",
    "import time\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-example-python')\\\n",
    "    .config('spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider','org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"def par: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "num_partitions = sc.defaultParallelism * 2\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "print()\n",
    "print(\"SQL JOIN\")\n",
    "print()\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "\n",
    "with get_time(\"Reading cens shp file\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "with get_time(\"Reading com shp file\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "    \n",
    "cens_df = Adapter.toDf(cens_rdd, sedona).repartition(num_partitions)\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "com_df = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns]).repartition(num_partitions)\n",
    "\n",
    "cens_df.createOrReplaceTempView(\"cens_view\")\n",
    "com_df.createOrReplaceTempView(\"com_view\")\n",
    "\n",
    "result = sedona.sql('''\n",
    "    SELECT cens.*, \n",
    "           com.*\n",
    "    FROM com_view com, cens_view cens\n",
    "    WHERE ST_Contains(com.geometry_com, cens.geometry)\n",
    "''')\n",
    "\n",
    "with get_time(\"toDf + rename cols + SQL views + Spatial Join\"):\n",
    "    result.write.format('geojson').mode('overwrite').save('./res_geojson')\n",
    "    # print(f\"Total features after join: {result.count()}\")\n",
    "\n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357ed19-f444-4ca4-a673-c93a5550b099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map = SedonaKepler.create_map(df=com_df, name=\"Comune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf79a4-b3f8-42ea-8251-b081b0d6761b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22020a2a-3b17-4648-8f90-0750106808fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2c20b-872d-4684-9d47-91036ee0876d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62f46e-84dd-47c0-aaac-675045e276a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0e68a-c70a-4ee8-a0a7-ff41806b057f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## DataFrame API spatial join, export to .shp\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col, expr\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "print(\"DataFrame API spatial join, export to .csv\")\n",
    "print()\n",
    "\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"   \n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "\n",
    "# Load into RDD: Eager operation \n",
    "with get_time(\"Reading comune shp to SpatialRDD\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "with get_time(\"Reading census shp to SpatialRDD\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "\n",
    "# com_rdd.rawSpatialRDD = com_rdd.rawSpatialRDD.repartition(4)\n",
    "# cens_rdd.rawSpatialRDD = cens_rdd.rawSpatialRDD.repartition(4)\n",
    "\n",
    "# RDD -> DataFrame: Lazy operation\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "cens_df = Adapter.toDf(cens_rdd, sedona)\n",
    "\n",
    "# Renaming columns: Lazy operation\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns])\n",
    "cens_df_renamed = cens_df.selectExpr(*[f\"{c} AS {c}_cens\" for c in cens_df.columns])\n",
    "\n",
    "# com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns]).repartition(8)\n",
    "# cens_df_renamed = cens_df.selectExpr(*[f\"{c} AS {c}_cens\" for c in cens_df.columns]).repartition(8)\n",
    "\n",
    "# SPATIAL JOIN\n",
    "# join, drop, withColumn, cache(): Lazy operatoons\n",
    "joined_df = (\n",
    "    com_df_renamed.select(\"COMUNE_com\", \"geometry_com\")\n",
    "    .alias(\"com\")\n",
    "    .join(\n",
    "        cens_df_renamed.alias(\"cens\"),\n",
    "        expr(\"ST_Contains(com.geometry_com, cens.geometry_cens)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "joined_df = joined_df.drop(\"geometry_com\")\n",
    "joined_df = joined_df.withColumn(\"geometry\", expr(\"ST_AsText(geometry_cens)\")).drop(\"geometry_cens\")\n",
    "# joined_df = joined_df.repartition(8)\n",
    "# joined_df.cache()\n",
    "\n",
    "# CHCKPT 1: `.write()` triggers execution of all \"Lazy operations\" above\n",
    "with get_time(\"Executing join and write to .shp\"):\n",
    "    pandas_df = joined_df.toPandas()\n",
    "    pandas_df[\"geometry\"] = pandas_df[\"geometry\"].apply(loads)\n",
    "    gdf = gpd.GeoDataFrame(pandas_df, geometry=\"geometry\")\n",
    "\n",
    "    # gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    gdf.to_file(\"./joined_shp\", driver=\"ESRI Shapefile\")\n",
    "    \n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "    \n",
    "print(f\"Total features after join: {joined_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d1bf3-65de-4125-a5df-cb4ba670172f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "com_rdd.rawSpatialRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e08ab7-3b61-4a68-8d83-66e9028e52a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## GeoPandas DF join\n",
    "# ## ISSUE: EXHAUSTS MEMORY\n",
    " \n",
    "# import geopandas as gpd\n",
    "# import time\n",
    "# from contextlib import contextmanager\n",
    "# from pyspark.sql.functions import col, expr\n",
    "\n",
    "# total_time = 0\n",
    "\n",
    "# @contextmanager\n",
    "# def get_time(task_name):\n",
    "#     start = time.time()\n",
    "#     yield\n",
    "#     elapsed = time.time() - start\n",
    "#     global total_time\n",
    "#     total_time += elapsed\n",
    "#     print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "# print(\"DF API join using GeoPandas\")\n",
    "# print()\n",
    "\n",
    "# # Load into RDD: Eager operation\n",
    "# with get_time(\"Reading comune shp to GeoPandas DF\"):\n",
    "#     com_gdf = gpd.read_file(\"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/Com01012024_WGS84.shp\")\n",
    "\n",
    "# with get_time(\"Reading census shp to GeoPandas DF\"):\n",
    "#     cens_gdf = gpd.read_file(\"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/R03_21_WGS84.shp\")\n",
    "\n",
    "# # GeoPandas DataFrame -> Sedona DataFrame: Lazy operation\n",
    "# com_sdf = sedona.createDataFrame(com_gdf)\n",
    "# cens_sdf = sedona.createDataFrame(cens_gdf)\n",
    "\n",
    "# # Renaming columns: Lazy operation\n",
    "# com_df_renamed = com_sdf.selectExpr(*[f\"{c} AS {c}_com\" for c in com_sdf.columns])  # Lazy\n",
    "# census_df_renamed = cens_sdf.selectExpr(*[f\"{c} AS {c}_cens\" for c in cens_sdf.columns])  # Lazy\n",
    "\n",
    "# # Spatial join\n",
    "# # join, drop, withColumn, cache():  Lazy operation\n",
    "# joined_df = com_df_renamed.alias(\"com\").join(\n",
    "#     census_df_renamed.alias(\"cens\"),\n",
    "#     expr(\"ST_Intersects(com.geometry_com, cens.geometry_cens)\"),\n",
    "#     \"inner\"\n",
    "# )\n",
    "# joined_df = joined_df.drop(\"geometry_com\")\n",
    "# joined_df = joined_df.withColumn(\"geometry\", expr(\"ST_AsText(geometry_cens)\")).drop(\"geometry_cens\")\n",
    "# joined_df.cache()\n",
    "\n",
    "# # CHCKPT 1: `.write()` triggers execution of all \"Lazy operations\" above\n",
    "# with get_time(\"Executing join and write to .csv\"):\n",
    "#     joined_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./gdf_joined_csv\")\n",
    "\n",
    "# print(f\"Total features aftre join: {joined_df.count()}\")\n",
    "\n",
    "# print(f\"Total time: {(total_time/60):.2f} min\" if total_time >= 60 else f\"Total time: {total_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa3357-0d10-46d7-a3f1-d6ae7f9a6bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SpatialRDD JOIN result -> DF -> .csv\n",
    "\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType  GridType.KDBTREE\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from pyspark.sql.functions import col, expr\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "print(\"SpatialRDD JOIN result -> DF -> .csv\")\n",
    "print()\n",
    "\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"   \n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "\n",
    "# Load into RDD: Eager operation \n",
    "with get_time(\"Reading comune shp to SpatialRDD\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, com_shp_path)\n",
    "with get_time(\"Reading census shp to SpatialRDD\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, cens_shp_path)\n",
    "    \n",
    "# .analyze(): calculates metadata like bounds, counts\n",
    "with get_time(\"com_rdd.analyze()\"):\n",
    "    com_rdd.analyze()\n",
    "with get_time(\"cens_rdd.analyze()\"):\n",
    "    cens_rdd.analyze()\n",
    "\n",
    "# Spatial Partitioning + Indexing: LAzy operations\n",
    "com_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "cens_rdd.spatialPartitioning(com_rdd.getPartitioner())\n",
    "com_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "cens_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "\n",
    "# RDD Join + toDf: Lazy operations\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    com_rdd, cens_rdd, useIndex=True, considerBoundaryIntersection=False\n",
    ")\n",
    "join_rdd_df = Adapter.toDf(join_result, com_rdd.fieldNames, cens_rdd.fieldNames, spark)\n",
    "\n",
    "def unique_cols(df):\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        lower_col = col_name.lower()\n",
    "        if lower_col in seen:\n",
    "            seen[lower_col] += 1\n",
    "            new_col = f\"{col_name}_dup_{seen[lower_col] - 1}\"\n",
    "        else:\n",
    "            seen[lower_col] = 1\n",
    "            new_col = col_name\n",
    "        new_cols.append(new_col)\n",
    "\n",
    "    df = df.toDF(*new_cols)\n",
    "    return df\n",
    "\n",
    "join_rdd_df = unique_cols(join_rdd_df)\n",
    "\n",
    "join_rdd_df = join_rdd_df.drop(\"geom_1\")\n",
    "# join_rdd_df = join_rdd_df.withColumn(\"geom2_\", expr(\"ST_AsText(geom_2)\")).drop(\"geom_2\")\n",
    "# # join_rdd_df.cache()\n",
    "\n",
    "# # CHCKPT 1: `.write()` triggers execution of all \"Lazy operations\" above\n",
    "# with get_time(\"Executing join and write to .csv\"):\n",
    "#     join_rdd_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./RDD_df_joined_csv\")\n",
    "\n",
    "# if((total_time)/60 < 1):\n",
    "#     print(f\"Job done in {total_time:.2f} sec\")\n",
    "# else:\n",
    "#     print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "    \n",
    "# print(f\"Total features after join: {join_rdd_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d261e0-45ec-40a7-8f01-220953338a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "join_rdd_df = join_rdd_df.drop(\"COD_ZIC\")\n",
    "join_rdd_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./RDD_df_joined_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fddd42-35c6-49f6-83d0-0a1cdfb51a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "join_rdd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91687113-3b0f-4341-8280-de07341abf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d438ae-63b1-4473-b5d9-a2347f089c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_join_rdd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6ad7e-6d52-434b-a57a-870970c2093b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join_rdd_df.rdd.getNumPartitions()\n",
    "join_rdd_df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eaba2b-71da-4f0a-99ef-c409e6a25e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be7f12-ac52-428d-a603-73945f73ddeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total time: {(total_time/60):.2f} min\" if total_time >= 60 else f\"{total_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4dd238-a1ca-4965-9bee-ab55bf1e3271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SpatialRDD JOIN + result to DF\n",
    "\n",
    "import time\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    com_rdd, census_rdd, useIndex=True, considerBoundaryIntersection=True\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "join_rdd_df = Adapter.toDf(join_result, com_rdd.fieldNames, census_rdd.fieldNames, spark)\n",
    "end = time.time()\n",
    "print(f\"toDf(): {(end-start):.2f} sec\")\n",
    "\n",
    "start = time.time()\n",
    "print(join_rdd_df.count())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"count(): {(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d29cdb-dd6e-4eb4-b297-6e08d027d923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d0103-61ff-4a4e-b802-4e1430c56af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af4f61-06ba-480f-8c36-f6d685bc4492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff0c10-1a85-4952-8c8b-c6b8673c09b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042bc86-30c7-45fc-afdb-6d78a68209d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13add3e3-a282-42bb-846b-9a630612bc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4540a7-72b0-421b-93a6-db892664fe84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b17a9-26e3-4651-b06d-c2f6c586ecd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## ShapeFile to SpatialRDD\n",
    "\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "# reg_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Reg01012024/\"\n",
    "# prov_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/ProvCM01012024/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "census_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "\n",
    "\n",
    "# reg_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, reg_shp_path)\n",
    "# prov_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, prov_shp_path)\n",
    "com_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, com_shp_path)\n",
    "census_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, census_shp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09810758-1387-4961-bbce-e47dc772ea16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## SpatialRDD to DF\n",
    "\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "\n",
    "# reg_df = Adapter.toDf(reg_rdd, spark)\n",
    "# prov_df = Adapter.toDf(prov_rdd, spark)\n",
    "com_df = Adapter.toDf(com_rdd, spark)\n",
    "census_df = Adapter.toDf(census_rdd, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389fa0d-0ce4-454b-9ae5-90555b350d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpatialRDD JOIN + result to DF\n",
    "\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    com_rdd, census_rdd, useIndex=True, considerBoundaryIntersection=True\n",
    ")\n",
    "\n",
    "join_rdd_df = Adapter.toDf(join_result, com_rdd.fieldNames, census_rdd.fieldNames, spark)\n",
    "\n",
    "start = time.time()\n",
    "print(join_rdd_df.count())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d40211-ceee-4918-afed-4da7857d00d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Spatial Partitiotning and  Indeing + SpatialRDD API operations\n",
    "\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "\n",
    "# reg_rdd.analyze()\n",
    "# prov_rdd.analyze()\n",
    "com_rdd.analyze()\n",
    "census_rdd.analyze()\n",
    "\n",
    "# reg_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "# prov_rdd.spatialPartitioning(reg_rdd.getPartitioner())\n",
    "com_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "census_rdd.spatialPartitioning(com_rdd.getPartitioner())\n",
    "\n",
    "# reg_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "# prov_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "com_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "census_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fb706-9213-40cf-ae6e-3075da5f6d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SpatialRDD JOIN + result to DF\n",
    "\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    com_rdd, census_rdd, useIndex=True, considerBoundaryIntersection=True\n",
    ")\n",
    "\n",
    "join_rdd_df = Adapter.toDf(join_result, com_rdd.fieldNames, census_rdd.fieldNames, spark)\n",
    "\n",
    "start = time.time()\n",
    "print(join_rdd_df.count())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05175e89-bfc9-4d6b-af96-c6e1c8b409bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## DataFrame API spatial join, export to .csv\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns])\n",
    "census_df_renamed = census_df.selectExpr(*[f\"{c} AS {c}_cens\" for c in census_df.columns])\n",
    "test = com_df_renamed.alias(\"com\").join(census_df_renamed.alias(\"cens\"), expr(\"ST_Intersects(com.geometry_com, cens.geometry_cens)\"), \"inner\")\n",
    "test = test.drop(\"geometry_com\")\n",
    "test = test.withColumn(\"geometry\", expr(\"ST_AsText(geometry_cens)\")).drop(\"geometry_cens\")\n",
    "# test = test.withColumn(\"geometry\", expr(\"ST_SetSRID(ST_GeomFromText(geometry), 4326)\"))\n",
    "\n",
    "start = time.time()\n",
    "# test.write.mode(\"overwrite\").csv(\"./df_joined_csv\")\n",
    "end = time.time()\n",
    "\n",
    "if((end-start)/60 < 1):\n",
    "    print(f\"{end-start:.2f} sec\")\n",
    "else:\n",
    "    print(f\"{(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e4afc-43c2-4155-9748-a4889585ea2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## SpatialRDD JOIN + result to DF\n",
    "\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    reg_rdd, census_rdd, useIndex=True, considerBoundaryIntersection=True\n",
    ")\n",
    "\n",
    "join_rdd_df = Adapter.toDf(join_result, reg_rdd.fieldNames, census_rdd.fieldNames, spark)\n",
    "\n",
    "start = time.time()\n",
    "print(join_rdd_df.count())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{(start-end)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e58831-8952-4360-a3f6-ba04c03f1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_join_df = reg_df.alias(\"reg\").join(prov_df.alias(\"prov\"), expr(\"reg.COD_REG == prov.COD_REG\"), \"inner\")\n",
    "\n",
    "polygon_df = joined_df.alias(\"reg\").filter(expr(\"ST_GeometryType(reg.geometry) = 'Polygon'\"))\n",
    "\n",
    "polygon_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
