{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b21a328-237b-430e-8dce-576ad78fbe4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/13 11:26:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "sc.defaultParallelism: 8\n",
      "using partitions: 8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"~Wherobots setup\"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-test').getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"sc.defaultParallelism: {sc.defaultParallelism}\")\n",
    "num_partitions = sc.defaultParallelism\n",
    "print(f\"using partitions: {num_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff076ce3-cc96-4a16-9d07-2f624b465891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m upload_jars()\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39m\\\n\u001b[1;32m      9\u001b[0m       getOrCreate()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mGeoSparkRegistrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregisterAll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/geospark/register/geo_registrator.py:26\u001b[0m, in \u001b[0;36mGeoSparkRegistrator.registerAll\u001b[0;34m(cls, spark)\u001b[0m\n\u001b[1;32m     24\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT 1 as geom\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     25\u001b[0m PackageImporter\u001b[38;5;241m.\u001b[39mimport_jvm_lib(spark\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/geospark/register/geo_registrator.py:31\u001b[0m, in \u001b[0;36mGeoSparkRegistrator.register\u001b[0;34m(cls, spark)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister\u001b[39m(\u001b[38;5;28mcls\u001b[39m, spark: SparkSession):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGeoSparkSQLRegistrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregisterAll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from geospark.register import upload_jars\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "\n",
    "upload_jars()\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "      getOrCreate()\n",
    "\n",
    "GeoSparkRegistrator.registerAll(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4401d-e0f1-4c38-92de-6205cf2ba34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635defc2-782c-4c0e-873d-9e702bca4004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# from sedona.spark import *\n",
    "# from sedona.spark import SedonaContext, Adapter\n",
    "# from contextlib import contextmanager\n",
    "# import time\n",
    "# from pyspark.sql.functions import col, expr\n",
    "# from pyspark.sql.functions import desc\n",
    "# from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "# config = SedonaContext.builder().appName('sedona-test').getOrCreate()\n",
    "\n",
    "# sedona = SedonaContext.create(config)\n",
    "# sc = sedona.sparkContext\n",
    "# print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "# print(f\"sc.defaultParallelism: {sc.defaultParallelism}\")\n",
    "\n",
    "# \"\"\"\"\"\"\n",
    "# num_partitions = sc.defaultParallelism\n",
    "# print(f\"using partitions: {num_partitions}\")\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType\n",
    "# from geospark.utils.adapter import Adapter\n",
    "\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "    \n",
    "def chudu(df):\n",
    "    \n",
    "    df = df.toPandas()\n",
    "    rows, cols = df.shape\n",
    "    print(f\"Total rows: {rows}, Total columns: {cols}\")\n",
    "    \n",
    "    memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage: {memory:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head(5))\n",
    "    \n",
    "    unique_fields = [col for col in df.columns if df[col].is_unique]\n",
    "    print(\"\\nCols with unique values no repeats:\")\n",
    "    print(unique_fields if unique_fields else \"None\")\n",
    "    \n",
    "    nan_columns = df.isna().sum()\n",
    "    nan_columns = nan_columns[nan_columns > 0]\n",
    "    print(\"\\nColumns with NaN:\")\n",
    "    print(nan_columns if not nan_columns.empty else \"None\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "# countries_path = \"./my_data/data_EU/countries_shp/\"\n",
    "# countries_rdd = ShapefileReader.readToGeometryRDD(sc, countries_path)\n",
    "\n",
    "italy_rdd = ShapefileReader.readToGeometryRDD(sc, \"./my_data/data_Italy/merged/merged_pop_geom/\")\n",
    "italy_df = Adapter.toDf(italy_rdd, sedona)\n",
    "\n",
    "# grids_df = sedona.read.parquet(\"./my_data/data_EU/census_grid_EU/grids.parquet\")\n",
    "# grids_df = grids_df.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n",
    "# grids_df = grids_df.withColumn(\n",
    "#     \"GRD_ID\", regexp_replace(\"GRD_ID\", \"CRS3035RES1000m\", \"\")\n",
    "# )\n",
    "# grids_df = grids_df.select(\"GRD_ID\", \"T\", \"geom\")\n",
    "# grids_rdd = Adapter.toSpatialRdd(grids_df)\n",
    "# grids_rdd.analyze()\n",
    "# grids_rdd.boundaryEnvelope\n",
    "\n",
    "# countries_rdd.analyze()\n",
    "\n",
    "# countries_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "# countries_rdd.spatialPartitioning(com_rdd.getPartitioner())\n",
    "# grids_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "# countries_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "\n",
    "# countries_df = Adapter.toDf(countries_rdd, sedona)\n",
    "# countries_df = countries_df.select(\"CNTR_ID\", \"NAME_ENGL\", \"geometry\")\n",
    "\n",
    "\n",
    "# grids_df = grids_df.withColumn(\"geom\", expr(\"ST_AsText(geom)\"))\n",
    "\n",
    "italy_df = italy_df.repartition(8)\n",
    "\n",
    "joined_df = italy_df.alias(\"it\").join(\n",
    "    italy_df.alias(\"itt\"),\n",
    "    expr(\"ST_Intersects(it.geometry, itt.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "# chudu(joined_df)\n",
    "\n",
    "# print(joined_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b43286-02a9-4067-8a6b-4f248405e5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame\n",
    "italy_df.cache()\n",
    "\n",
    "# Perform an action to trigger caching\n",
    "italy_df.count()\n",
    "\n",
    "# Use the Spark UI or log to see the size\n",
    "sedona.catalog._jvm.org.apache.spark.storage.StorageUtils.printDiskUsage(.sparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47942363-96ac-4cb9-baa5-0a484dd0532e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import regexp_replace\n",
    "# from sedona.sql.types import GeometryType\n",
    "\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from sedona.sql.types import GeometryType\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-test').getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"sc.defaultParallelism: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "num_partitions = sc.defaultParallelism\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "    \n",
    "def chudu(df):\n",
    "    \n",
    "    df = df.toPandas()\n",
    "    rows, cols = df.shape\n",
    "    print(f\"Total rows: {rows}, Total columns: {cols}\")\n",
    "    \n",
    "    memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage: {memory:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head(5))\n",
    "    \n",
    "    unique_fields = [col for col in df.columns if df[col].is_unique]\n",
    "    print(\"\\nCols with unique values no repeats:\")\n",
    "    print(unique_fields if unique_fields else \"None\")\n",
    "    \n",
    "    nan_columns = df.isna().sum()\n",
    "    nan_columns = nan_columns[nan_columns > 0]\n",
    "    print(\"\\nColumns with NaN:\")\n",
    "    print(nan_columns if not nan_columns.empty else \"None\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "countries_path = \"./my_data/data_EU/countries_shp/\"\n",
    "countries_rdd = ShapefileReader.readToGeometryRDD(sc, countries_path)\n",
    "countries_df = Adapter.toDf(countries_rdd, sedona)\n",
    "\n",
    "grids_df = sedona.read.parquet(\"./my_data/data_EU/census_grid_EU/grids.parquet\")\n",
    "# grids_df = grids_df.withColumn(\"geom\", grids_df[\"geom\"].cast(GeometryType))\n",
    "grids_df = grids_df.withColumn(\"geom\", expr(\"ST_GeomFromWKB(geom)\"))\n",
    "\n",
    "# grids_df = grids_df.withColumn(\"geom\", expr(\"ST_AsText(geom)\"))\n",
    "\n",
    "grids_df = grids_df.withColumn(\n",
    "    \"GRD_ID\", regexp_replace(\"GRD_ID\", \"CRS3035RES1000m\", \"\")\n",
    ")\n",
    "grids_df = grids_df.select(\"GRD_ID\", \"T\", \"geom\")\n",
    "countries_df = countries_df.select(\"CNTR_ID\", \"NAME_ENGL\", \"geometry\")\n",
    "\n",
    "grids_df = grids_df.repartition(16)\n",
    "\n",
    "joined_df = grids_df.alias(\"grd\").join(\n",
    "    countries_df.alias(\"con\"),\n",
    "    expr(\"ST_Intersects(grd.geom, con.geometry)\"),\n",
    "    \"inner\"\n",
    ")\n",
    "# chudu(joined_df)\n",
    "\n",
    "print(joined_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0241cf-703d-47cb-aca5-f7fa7d03ecca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grids_df.printSchema()\n",
    "countries_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffb5465d-60b7-44c9-9525-b599f7ff0a36",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c4c915-00ac-4078-99ca-ef0f657e02d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/07 11:28:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "def par: 8\n",
      "using partitions: 16\n",
      "\n",
      "DF JOIN\n",
      "\n",
      "Reading cens shp file... DONE in 0.52 sec\n",
      "Reading com shp file... DONE in 0.31 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Partitioning... DONE in 5.68 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 11:28:51 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toDf + rename cols + Spatial Join... DONE in 50.42 sec\n",
      "Job done in 56.93 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" SPATIAL PARTITIOING. DF JOIN \"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "import time\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-example-python')\\\n",
    "    .config('spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider','org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"def par: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "# num_partitions = sc.defaultParallelism * 2\n",
    "num_partitions = 16\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "print()\n",
    "print(\"DF JOIN\")\n",
    "print()\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "\n",
    "with get_time(\"Reading cens shp file\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "with get_time(\"Reading com shp file\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "    \n",
    "\n",
    "with get_time(\"Spatial Partitioning\"):\n",
    "    cens_rdd.analyze()\n",
    "    com_rdd.analyze()\n",
    "\n",
    "    cens_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "    com_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "    \n",
    "cens_df = Adapter.toDf(cens_rdd, sedona)\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns])\n",
    "\n",
    "joined_df = (\n",
    "    com_df_renamed.select(\"COMUNE_com\", \"geometry_com\")\n",
    "    .alias(\"com\")\n",
    "    .join(\n",
    "        cens_df.alias(\"cens\"),\n",
    "        expr(\"ST_Contains(com.geometry_com, cens.geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with get_time(\"toDf + rename cols + Spatial Join\"):\n",
    "    joined_df.write.format('geoparquet').mode('overwrite').save('./res_geoparquet')\n",
    "    # print(f\"Total features after join: {joined_df.count()}\")\n",
    "\n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e14f8e-c824-48b6-a980-8b338032890c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb10d04-aa36-430b-8f59-ab63dfee0ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd534e4a-382f-4d66-9946-a17a7e05aaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88025cf7-fb68-4e0e-881b-58df54a38bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/07 11:42:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "def par: 8\n",
      "using partitions: 8\n",
      "\n",
      "DF JOIN\n",
      "\n",
      "Reading cens shp file... DONE in 0.47 sec\n",
      "Reading com shp file... DONE in 0.29 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/07 11:42:47 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toDf + rename cols + Spatial Join... DONE in 29.21 sec\n",
      "Job done in 29.98 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" repartition(8, \"column\") DF JOIN \"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "import time\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-example-python')\\\n",
    "    .config('spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider','org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"def par: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "# num_partitions = sc.defaultParallelism * 2\n",
    "num_partitions = 8\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "print()\n",
    "print(\"DF JOIN\")\n",
    "print()\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "\n",
    "with get_time(\"Reading cens shp file\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "with get_time(\"Reading com shp file\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "    \n",
    "cens_df = Adapter.toDf(cens_rdd, sedona).repartition(num_partitions, \"COD_UTS\")\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns]).repartition(num_partitions, \"COD_UTS_com\")\n",
    "\n",
    "joined_df = (\n",
    "    com_df_renamed.select(\"COMUNE_com\", \"geometry_com\")\n",
    "    .alias(\"com\")\n",
    "    .join(\n",
    "        cens_df.alias(\"cens\"),\n",
    "        expr(\"ST_Contains(com.geometry_com, cens.geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with get_time(\"toDf + rename cols + Spatial Join\"):\n",
    "    joined_df.write.format('geoparquet').mode('overwrite').save('./res_geoparquet')\n",
    "    # print(f\"Total features after join: {joined_df.count()}\")\n",
    "\n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7084bce1-9f4d-4349-82bb-6733492d2864",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------+-------+-------+------------+----------+--------+----------+-------+--------+---------+---------+----------+----------+--------+--------+--------+-----+-----+-----------------+-----------------+\n",
      "|            geometry|COD_REG|COD_UTS|PRO_COM|  SEZ21|    SEZ21_ID|COD_TIPO_S|TIPO_LOC|  LOC21_ID|COD_ZIC|COD_ISAM|COD_ACQUE|COD_ISOLE|COD_MONT_D|COD_AREA_S|COM_ASC1|COM_ASC2|COM_ASC3|POP21|FAM21|       SHAPE_Leng|       SHAPE_Area|\n",
      "+--------------------+-------+-------+-------+-------+------------+----------+--------+----------+-------+--------+---------+---------+----------+----------+--------+--------+--------+-----+-----+-----------------+-----------------+\n",
      "|POLYGON ((516786....|      3|    215|  15146|   1528|151460001528|         1|       1|1514610006|      0|       0|        0|        0|         0|         0|15146003|15146021|       0|  197|  120|4.58618956384e+02|8.24104927375e+03|\n",
      "|POLYGON ((539453....|      3|     98|  98031|     13|980310000013|         1|       1|9803110002|      0|       0|        0|        0|         0|         0|       0|       0|       0|   99|   41|5.70214226132e+02|1.14522615799e+04|\n",
      "|POLYGON ((592861....|      3|     17|  17065|     71|170650000071|        15|       1|1706510002|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|2.48113235065e+02|3.81125417606e+03|\n",
      "|POLYGON ((531285....|      3|     97|  97058|8888888|970588888888|       100|       1|9705810001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|4.73694009160e+01|1.32729400947e+02|\n",
      "|POLYGON ((540136....|      3|    215|  15059|     98|150590000098|        16|       1|1505910002|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|3.74352845116e+02|8.23159813350e+03|\n",
      "|POLYGON ((518321....|      3|    215|  15146|   1825|151460001825|         1|       1|1514610006|      0|       0|        0|        0|         0|         0|15146003|15146018|       0|  208|  124|5.27667123324e+02|8.73566070054e+03|\n",
      "|POLYGON ((640513....|      3|     20|  20030|    623|200300000623|         5|       1|2003010005|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|2.33580457678e+02|3.43598653150e+03|\n",
      "|POLYGON ((494640....|      3|    215|  15085|     68|150850000068|         1|       1|1508510001|      0|       0|        0|        0|         0|         0|       0|       0|       0|  273|  108|1.09836466730e+03|3.18341670015e+04|\n",
      "|POLYGON ((499253....|      3|     18|  18182|    336|181820000336|        12|       1|1818210004|      0|       0|        0|        0|         0|         0|       0|       0|       0|    3|    1|1.23526080623e+03|6.31057028918e+04|\n",
      "|POLYGON ((551390....|      3|     98|  98002|      8|980020000008|         1|       2|9800226604|      0|       0|        0|        0|         0|         0|       0|       0|       0|   38|   13|9.97191253953e+02|3.03137670449e+04|\n",
      "|POLYGON ((512953....|      3|    215|  15146|   6348|151460006348|        36|       1|1514610006|      0|       0|        0|        0|         0|         0|15146009|15146077|       0|    0|    0|1.97039441275e+03|1.51370234268e+04|\n",
      "|POLYGON ((548192....|      3|     16|  16053|    156|160530000156|        36|       4|1605340001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|2.87260727013e+03|1.81640689305e+04|\n",
      "|POLYGON ((502422....|      3|     13|  13036|     49|130360000049|        36|       4|1303640001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|1.85763464534e+03|3.15170147399e+04|\n",
      "|POLYGON ((590284....|      3|     17|  17072|     13|170720000013|        12|       1|1707210002|      0|       0|        0|        0|         0|         0|       0|       0|       0|  117|   43|4.99574936024e+03|6.14598210415e+05|\n",
      "|POLYGON ((502305....|      3|     18|  18091|     11|180910000011|        29|       1|1809110001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|1.24943930979e+02|9.28696910963e+02|\n",
      "|POLYGON ((513430....|      3|    215|  15146|   5814|151460005814|         1|       1|1514610006|      0|       0|        0|        0|         0|         0|15146009|15146083|       0|  319|  144|5.83467101560e+02|1.51437382926e+04|\n",
      "|POLYGON ((488605....|      3|     18|  18177|     15|181770000015|         1|       1|1817710003|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|2.59655638304e+02|2.70614338613e+03|\n",
      "|POLYGON ((507021....|      3|     18|  18126|     27|181260000027|        15|       4|1812640001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|3.48805919358e+02|5.05267147528e+03|\n",
      "|POLYGON ((504838....|      3|    215|  15211|    180|152110000180|         1|       4|1521140001|      0|       0|        0|        0|         0|         0|       0|       0|       0|    1|    1|2.97883895693e+02|5.40174826644e+03|\n",
      "|POLYGON ((528045....|      3|     14|  14058|     20|140580000020|         1|       2|1405824912|      0|       0|        0|        0|         0|         0|       0|       0|       0|    0|    0|7.95160576592e+02|2.24681827157e+04|\n",
      "+--------------------+-------+-------+-------+-------+------------+----------+--------+----------+-------+--------+---------+---------+----------+----------+--------+--------+--------+-----+-----+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cens_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cebd556-e6d4-4125-b18f-a35a602e6a54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------+--------+------+-------+-------+---------+-------------------+--------+------+------------------+------------------+\n",
      "|            geometry|COD_RIP|COD_REG|COD_PROV|COD_CM|COD_UTS|PRO_COM|PRO_COM_T|             COMUNE|COMUNE_A|CC_UTS|        Shape_Leng|        Shape_Area|\n",
      "+--------------------+-------+-------+--------+------+-------+-------+---------+-------------------+--------+------+------------------+------------------+\n",
      "|POLYGON ((404703....|      1|      1|       1|     0|    201|   1001|   001001|             AgliÃ¨|       0|     0|1.80352540007e+004|1.31462576661e+007|\n",
      "|POLYGON ((380700....|      1|      1|       1|     0|    201|   1002|   001002|            Airasca|       0|     0|1.84089069884e+004|1.57393133213e+007|\n",
      "|POLYGON ((364710....|      1|      1|       1|     0|    201|   1003|   001003|       Ala di Stura|       0|     0|3.18341560810e+004|4.63315602074e+007|\n",
      "|POLYGON ((415942....|      1|      1|       1|     0|    201|   1004|   001004|    Albiano d'Ivrea|       0|     0|1.89272628270e+004|1.17396902250e+007|\n",
      "|POLYGON ((376934....|      1|      1|       1|     0|    201|   1006|   001006|             Almese|       0|     0|1.70574139642e+004|1.78741245214e+007|\n",
      "|POLYGON ((388890....|      1|      1|       1|     0|    201|   1007|   001007|            Alpette|       0|     0|9.74254551158e+003|5.63111028264e+006|\n",
      "|POLYGON ((382213....|      1|      1|       1|     0|    201|   1008|   001008|          Alpignano|       0|     0|1.79831548922e+004|1.19192532455e+007|\n",
      "|POLYGON ((410098....|      1|      1|       1|     0|    201|   1009|   001009|           Andezeno|       0|     0|1.30913340003e+004|7.48592427309e+006|\n",
      "|POLYGON ((413793....|      1|      1|       1|     0|    201|   1010|   001010|            Andrate|       0|     0|2.01374964019e+004|9.30899805827e+006|\n",
      "|POLYGON ((353807....|      1|      1|       1|     0|    201|   1011|   001011|           Angrogna|       0|     0|3.54417027386e+004|3.88781376267e+007|\n",
      "|POLYGON ((413545....|      1|      1|       1|     0|    201|   1012|   001012|           Arignano|       0|     0|1.40963290967e+004|8.16656007288e+006|\n",
      "|POLYGON ((373944....|      1|      1|       1|     0|    201|   1013|   001013|          Avigliana|       0|     0|2.58744703804e+004|2.32172747995e+007|\n",
      "|MULTIPOLYGON (((4...|      1|      1|       1|     0|    201|   1014|   001014|            Azeglio|       0|     0|2.25490384802e+004|9.95282100890e+006|\n",
      "|POLYGON ((402490....|      1|      1|       1|     0|    201|   1015|   001015|              Bairo|       0|     0|1.28614749609e+004|7.08593261854e+006|\n",
      "|POLYGON ((382311....|      1|      1|       1|     0|    201|   1016|   001016|          Balangero|       0|     0|1.84385104334e+004|1.30065097374e+007|\n",
      "|POLYGON ((402346....|      1|      1|       1|     0|    201|   1017|   001017|Baldissero Canavese|       0|     0|1.06787807036e+004|4.50640451054e+006|\n",
      "|POLYGON ((404596....|      1|      1|       1|     0|    201|   1018|   001018|Baldissero Torinese|       0|     0|2.20832482549e+004|1.54045666958e+007|\n",
      "|POLYGON ((356578....|      1|      1|       1|     0|    201|   1019|   001019|              Balme|       0|     0|3.71599832512e+004|6.27104944878e+007|\n",
      "|MULTIPOLYGON (((4...|      1|      1|       1|     0|    201|   1020|   001020|          Banchette|       0|     0|1.38601654299e+004|2.02889414066e+006|\n",
      "|POLYGON ((392550....|      1|      1|       1|     0|    201|   1021|   001021|           Barbania|       0|     0|1.64824605468e+004|1.28027373603e+007|\n",
      "+--------------------+-------+-------+--------+------+-------+-------+---------+-------------------+--------+------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "com_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3f4c32-a5a0-4b58-9a3b-6fea13b1a2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions in joined_df: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 15:47:35 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Partitions in joined_df: {joined_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0c584f-79be-46c4-abc3-6db31ccfe5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/05 16:12:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "def par: 8\n",
      "using partitions: 16\n",
      "\n",
      "DF JOIN\n",
      "\n",
      "Reading cens shp file... DONE in 0.48 sec\n",
      "Reading com shp file... DONE in 0.24 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 16:12:16 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toDf + rename cols + Spatial Join... DONE in 52.30 sec\n",
      "Job done in 53.02 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" DF JOIN \"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import desc\n",
    "import time\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-example-python')\\\n",
    "    .config('spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider','org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"def par: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "num_partitions = sc.defaultParallelism * 2\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "print()\n",
    "print(\"DF JOIN\")\n",
    "print()\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "\n",
    "with get_time(\"Reading cens shp file\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "with get_time(\"Reading com shp file\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "    \n",
    "cens_df = Adapter.toDf(cens_rdd, sedona)\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns])\n",
    "\n",
    "joined_df = (\n",
    "    com_df_renamed.select(\"COMUNE_com\", \"geometry_com\")\n",
    "    .alias(\"com\")\n",
    "    .join(\n",
    "        cens_df.alias(\"cens\"),\n",
    "        expr(\"ST_Contains(com.geometry_com, cens.geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with get_time(\"toDf + rename cols + Spatial Join\"):\n",
    "    joined_df.write.format('geoparquet').mode('overwrite').save('./res_geoparquet')\n",
    "    # print(f\"Total features after join: {joined_df.count()}\")\n",
    "\n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c377d5-699f-4f29-9b9a-4dbcc569ab08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/05 14:47:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master: local[*]\n",
      "def par: 8\n",
      "using partitions: 16\n",
      "\n",
      "SQL JOIN\n",
      "\n",
      "Reading cens shp file... DONE in 0.37 sec\n",
      "Reading com shp file... DONE in 0.19 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 14:48:01 WARN JoinQuery: UseIndex is true, but no index exists. Will build index on the fly.\n",
      "[Stage 17:=====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features after join: 99886\n",
      "toDf + rename cols + SQL views + Spatial Join... DONE in 13.98 sec\n",
      "Job done in 14.54 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" SQL JOIN \"\"\"\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import desc\n",
    "import time\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "config = SedonaContext.builder().appName('sedona-example-python')\\\n",
    "    .config('spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider','org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext\n",
    "print(f'master: {sedona.conf.get(\"spark.master\")}')\n",
    "print(f\"def par: {sc.defaultParallelism}\")\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "num_partitions = sc.defaultParallelism * 2\n",
    "print(f\"using partitions: {num_partitions}\")\n",
    "\n",
    "print()\n",
    "print(\"SQL JOIN\")\n",
    "print()\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "\n",
    "with get_time(\"Reading cens shp file\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "with get_time(\"Reading com shp file\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "    \n",
    "cens_df = Adapter.toDf(cens_rdd, sedona).repartition(num_partitions)\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "com_df = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns]).repartition(num_partitions)\n",
    "\n",
    "cens_df.createOrReplaceTempView(\"cens_view\")\n",
    "com_df.createOrReplaceTempView(\"com_view\")\n",
    "\n",
    "result = sedona.sql('''\n",
    "    SELECT cens.*, \n",
    "           com.*\n",
    "    FROM com_view com, cens_view cens\n",
    "    WHERE ST_Contains(com.geometry_com, cens.geometry)\n",
    "''')\n",
    "\n",
    "with get_time(\"toDf + rename cols + SQL views + Spatial Join\"):\n",
    "    result.write.format('geojson').mode('overwrite').save('./res_geojson')\n",
    "    # print(f\"Total features after join: {result.count()}\")\n",
    "\n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357ed19-f444-4ca4-a673-c93a5550b099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map = SedonaKepler.create_map(df=com_df, name=\"Comune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf79a4-b3f8-42ea-8251-b081b0d6761b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22020a2a-3b17-4648-8f90-0750106808fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2c20b-872d-4684-9d47-91036ee0876d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62f46e-84dd-47c0-aaac-675045e276a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0e68a-c70a-4ee8-a0a7-ff41806b057f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## DataFrame API spatial join, export to .shp\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col, expr\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "print(\"DataFrame API spatial join, export to .csv\")\n",
    "print()\n",
    "\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"   \n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "\n",
    "# Load into RDD: Eager operation \n",
    "with get_time(\"Reading comune shp to SpatialRDD\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(sc, com_shp_path)\n",
    "with get_time(\"Reading census shp to SpatialRDD\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(sc, cens_shp_path)\n",
    "\n",
    "\n",
    "# com_rdd.rawSpatialRDD = com_rdd.rawSpatialRDD.repartition(4)\n",
    "# cens_rdd.rawSpatialRDD = cens_rdd.rawSpatialRDD.repartition(4)\n",
    "\n",
    "# RDD -> DataFrame: Lazy operation\n",
    "com_df = Adapter.toDf(com_rdd, sedona)\n",
    "cens_df = Adapter.toDf(cens_rdd, sedona)\n",
    "\n",
    "# Renaming columns: Lazy operation\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns])\n",
    "cens_df_renamed = cens_df.selectExpr(*[f\"{c} AS {c}_cens\" for c in cens_df.columns])\n",
    "\n",
    "# com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns]).repartition(8)\n",
    "# cens_df_renamed = cens_df.selectExpr(*[f\"{c} AS {c}_cens\" for c in cens_df.columns]).repartition(8)\n",
    "\n",
    "# SPATIAL JOIN\n",
    "# join, drop, withColumn, cache(): Lazy operatoons\n",
    "joined_df = (\n",
    "    com_df_renamed.select(\"COMUNE_com\", \"geometry_com\")\n",
    "    .alias(\"com\")\n",
    "    .join(\n",
    "        cens_df_renamed.alias(\"cens\"),\n",
    "        expr(\"ST_Contains(com.geometry_com, cens.geometry_cens)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "joined_df = joined_df.drop(\"geometry_com\")\n",
    "joined_df = joined_df.withColumn(\"geometry\", expr(\"ST_AsText(geometry_cens)\")).drop(\"geometry_cens\")\n",
    "# joined_df = joined_df.repartition(8)\n",
    "# joined_df.cache()\n",
    "\n",
    "# CHCKPT 1: `.write()` triggers execution of all \"Lazy operations\" above\n",
    "with get_time(\"Executing join and write to .shp\"):\n",
    "    pandas_df = joined_df.toPandas()\n",
    "    pandas_df[\"geometry\"] = pandas_df[\"geometry\"].apply(loads)\n",
    "    gdf = gpd.GeoDataFrame(pandas_df, geometry=\"geometry\")\n",
    "\n",
    "    # gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    gdf.to_file(\"./joined_shp\", driver=\"ESRI Shapefile\")\n",
    "    \n",
    "if((total_time)/60 < 1):\n",
    "    print(f\"Job done in {total_time:.2f} sec\")\n",
    "else:\n",
    "    print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "    \n",
    "print(f\"Total features after join: {joined_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d1bf3-65de-4125-a5df-cb4ba670172f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "com_rdd.rawSpatialRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e08ab7-3b61-4a68-8d83-66e9028e52a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## GeoPandas DF join\n",
    "# ## ISSUE: EXHAUSTS MEMORY\n",
    " \n",
    "# import geopandas as gpd\n",
    "# import time\n",
    "# from contextlib import contextmanager\n",
    "# from pyspark.sql.functions import col, expr\n",
    "\n",
    "# total_time = 0\n",
    "\n",
    "# @contextmanager\n",
    "# def get_time(task_name):\n",
    "#     start = time.time()\n",
    "#     yield\n",
    "#     elapsed = time.time() - start\n",
    "#     global total_time\n",
    "#     total_time += elapsed\n",
    "#     print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "# print(\"DF API join using GeoPandas\")\n",
    "# print()\n",
    "\n",
    "# # Load into RDD: Eager operation\n",
    "# with get_time(\"Reading comune shp to GeoPandas DF\"):\n",
    "#     com_gdf = gpd.read_file(\"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/Com01012024_WGS84.shp\")\n",
    "\n",
    "# with get_time(\"Reading census shp to GeoPandas DF\"):\n",
    "#     cens_gdf = gpd.read_file(\"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/R03_21_WGS84.shp\")\n",
    "\n",
    "# # GeoPandas DataFrame -> Sedona DataFrame: Lazy operation\n",
    "# com_sdf = sedona.createDataFrame(com_gdf)\n",
    "# cens_sdf = sedona.createDataFrame(cens_gdf)\n",
    "\n",
    "# # Renaming columns: Lazy operation\n",
    "# com_df_renamed = com_sdf.selectExpr(*[f\"{c} AS {c}_com\" for c in com_sdf.columns])  # Lazy\n",
    "# census_df_renamed = cens_sdf.selectExpr(*[f\"{c} AS {c}_cens\" for c in cens_sdf.columns])  # Lazy\n",
    "\n",
    "# # Spatial join\n",
    "# # join, drop, withColumn, cache():  Lazy operation\n",
    "# joined_df = com_df_renamed.alias(\"com\").join(\n",
    "#     census_df_renamed.alias(\"cens\"),\n",
    "#     expr(\"ST_Intersects(com.geometry_com, cens.geometry_cens)\"),\n",
    "#     \"inner\"\n",
    "# )\n",
    "# joined_df = joined_df.drop(\"geometry_com\")\n",
    "# joined_df = joined_df.withColumn(\"geometry\", expr(\"ST_AsText(geometry_cens)\")).drop(\"geometry_cens\")\n",
    "# joined_df.cache()\n",
    "\n",
    "# # CHCKPT 1: `.write()` triggers execution of all \"Lazy operations\" above\n",
    "# with get_time(\"Executing join and write to .csv\"):\n",
    "#     joined_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./gdf_joined_csv\")\n",
    "\n",
    "# print(f\"Total features aftre join: {joined_df.count()}\")\n",
    "\n",
    "# print(f\"Total time: {(total_time/60):.2f} min\" if total_time >= 60 else f\"Total time: {total_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa3357-0d10-46d7-a3f1-d6ae7f9a6bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SpatialRDD JOIN result -> DF -> .csv\n",
    "\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType  GridType.KDBTREE\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from pyspark.sql.functions import col, expr\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "@contextmanager\n",
    "def get_time(task_name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    global total_time\n",
    "    total_time += elapsed\n",
    "    print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "          if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "print(\"SpatialRDD JOIN result -> DF -> .csv\")\n",
    "print()\n",
    "\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"   \n",
    "cens_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "\n",
    "# Load into RDD: Eager operation \n",
    "with get_time(\"Reading comune shp to SpatialRDD\"):\n",
    "    com_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, com_shp_path)\n",
    "with get_time(\"Reading census shp to SpatialRDD\"):\n",
    "    cens_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, cens_shp_path)\n",
    "    \n",
    "# .analyze(): calculates metadata like bounds, counts\n",
    "with get_time(\"com_rdd.analyze()\"):\n",
    "    com_rdd.analyze()\n",
    "with get_time(\"cens_rdd.analyze()\"):\n",
    "    cens_rdd.analyze()\n",
    "\n",
    "# Spatial Partitioning + Indexing: LAzy operations\n",
    "com_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "cens_rdd.spatialPartitioning(com_rdd.getPartitioner())\n",
    "com_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "cens_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "\n",
    "# RDD Join + toDf: Lazy operations\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    com_rdd, cens_rdd, useIndex=True, considerBoundaryIntersection=False\n",
    ")\n",
    "join_rdd_df = Adapter.toDf(join_result, com_rdd.fieldNames, cens_rdd.fieldNames, spark)\n",
    "\n",
    "def unique_cols(df):\n",
    "    seen = {}\n",
    "    new_cols = []\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        lower_col = col_name.lower()\n",
    "        if lower_col in seen:\n",
    "            seen[lower_col] += 1\n",
    "            new_col = f\"{col_name}_dup_{seen[lower_col] - 1}\"\n",
    "        else:\n",
    "            seen[lower_col] = 1\n",
    "            new_col = col_name\n",
    "        new_cols.append(new_col)\n",
    "\n",
    "    df = df.toDF(*new_cols)\n",
    "    return df\n",
    "\n",
    "join_rdd_df = unique_cols(join_rdd_df)\n",
    "\n",
    "join_rdd_df = join_rdd_df.drop(\"geom_1\")\n",
    "# join_rdd_df = join_rdd_df.withColumn(\"geom2_\", expr(\"ST_AsText(geom_2)\")).drop(\"geom_2\")\n",
    "# # join_rdd_df.cache()\n",
    "\n",
    "# # CHCKPT 1: `.write()` triggers execution of all \"Lazy operations\" above\n",
    "# with get_time(\"Executing join and write to .csv\"):\n",
    "#     join_rdd_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./RDD_df_joined_csv\")\n",
    "\n",
    "# if((total_time)/60 < 1):\n",
    "#     print(f\"Job done in {total_time:.2f} sec\")\n",
    "# else:\n",
    "#     print(f\"Job done in {(total_time)/60:.2f} min\")\n",
    "    \n",
    "# print(f\"Total features after join: {join_rdd_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d261e0-45ec-40a7-8f01-220953338a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "join_rdd_df = join_rdd_df.drop(\"COD_ZIC\")\n",
    "join_rdd_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./RDD_df_joined_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fddd42-35c6-49f6-83d0-0a1cdfb51a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "join_rdd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91687113-3b0f-4341-8280-de07341abf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d438ae-63b1-4473-b5d9-a2347f089c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_join_rdd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6ad7e-6d52-434b-a57a-870970c2093b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join_rdd_df.rdd.getNumPartitions()\n",
    "join_rdd_df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eaba2b-71da-4f0a-99ef-c409e6a25e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be7f12-ac52-428d-a603-73945f73ddeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total time: {(total_time/60):.2f} min\" if total_time >= 60 else f\"{total_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4dd238-a1ca-4965-9bee-ab55bf1e3271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SpatialRDD JOIN + result to DF\n",
    "\n",
    "import time\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    com_rdd, census_rdd, useIndex=True, considerBoundaryIntersection=True\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "join_rdd_df = Adapter.toDf(join_result, com_rdd.fieldNames, census_rdd.fieldNames, spark)\n",
    "end = time.time()\n",
    "print(f\"toDf(): {(end-start):.2f} sec\")\n",
    "\n",
    "start = time.time()\n",
    "print(join_rdd_df.count())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"count(): {(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d29cdb-dd6e-4eb4-b297-6e08d027d923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d0103-61ff-4a4e-b802-4e1430c56af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af4f61-06ba-480f-8c36-f6d685bc4492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff0c10-1a85-4952-8c8b-c6b8673c09b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042bc86-30c7-45fc-afdb-6d78a68209d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13add3e3-a282-42bb-846b-9a630612bc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4540a7-72b0-421b-93a6-db892664fe84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b17a9-26e3-4651-b06d-c2f6c586ecd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## ShapeFile to SpatialRDD\n",
    "\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "\n",
    "# reg_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Reg01012024/\"\n",
    "# prov_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/ProvCM01012024/\"\n",
    "com_shp_path = \"/opt/workspace/my_data/Dati Use Case/Limiti01012024/Com01012024/\"\n",
    "census_shp_path = \"/opt/workspace/my_data/Dati Use Case/Sezioni_censimento_Lombardia_R03_21/SHP/\"\n",
    "\n",
    "\n",
    "# reg_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, reg_shp_path)\n",
    "# prov_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, prov_shp_path)\n",
    "com_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, com_shp_path)\n",
    "census_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, census_shp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09810758-1387-4961-bbce-e47dc772ea16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## SpatialRDD to DF\n",
    "\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "\n",
    "# reg_df = Adapter.toDf(reg_rdd, spark)\n",
    "# prov_df = Adapter.toDf(prov_rdd, spark)\n",
    "com_df = Adapter.toDf(com_rdd, spark)\n",
    "census_df = Adapter.toDf(census_rdd, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389fa0d-0ce4-454b-9ae5-90555b350d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpatialRDD JOIN + result to DF\n",
    "\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    com_rdd, census_rdd, useIndex=True, considerBoundaryIntersection=True\n",
    ")\n",
    "\n",
    "join_rdd_df = Adapter.toDf(join_result, com_rdd.fieldNames, census_rdd.fieldNames, spark)\n",
    "\n",
    "start = time.time()\n",
    "print(join_rdd_df.count())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d40211-ceee-4918-afed-4da7857d00d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Spatial Partitiotning and  Indeing + SpatialRDD API operations\n",
    "\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType\n",
    "from sedona.spark import SedonaContext, Adapter\n",
    "\n",
    "# reg_rdd.analyze()\n",
    "# prov_rdd.analyze()\n",
    "com_rdd.analyze()\n",
    "census_rdd.analyze()\n",
    "\n",
    "# reg_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "# prov_rdd.spatialPartitioning(reg_rdd.getPartitioner())\n",
    "com_rdd.spatialPartitioning(GridType.KDBTREE)\n",
    "census_rdd.spatialPartitioning(com_rdd.getPartitioner())\n",
    "\n",
    "# reg_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "# prov_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "com_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)\n",
    "census_rdd.buildIndex(IndexType.RTREE, buildIndexOnSpatialPartitionedRDD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fb706-9213-40cf-ae6e-3075da5f6d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SpatialRDD JOIN + result to DF\n",
    "\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    com_rdd, census_rdd, useIndex=True, considerBoundaryIntersection=True\n",
    ")\n",
    "\n",
    "join_rdd_df = Adapter.toDf(join_result, com_rdd.fieldNames, census_rdd.fieldNames, spark)\n",
    "\n",
    "start = time.time()\n",
    "print(join_rdd_df.count())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05175e89-bfc9-4d6b-af96-c6e1c8b409bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## DataFrame API spatial join, export to .csv\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "com_df_renamed = com_df.selectExpr(*[f\"{c} AS {c}_com\" for c in com_df.columns])\n",
    "census_df_renamed = census_df.selectExpr(*[f\"{c} AS {c}_cens\" for c in census_df.columns])\n",
    "test = com_df_renamed.alias(\"com\").join(census_df_renamed.alias(\"cens\"), expr(\"ST_Intersects(com.geometry_com, cens.geometry_cens)\"), \"inner\")\n",
    "test = test.drop(\"geometry_com\")\n",
    "test = test.withColumn(\"geometry\", expr(\"ST_AsText(geometry_cens)\")).drop(\"geometry_cens\")\n",
    "# test = test.withColumn(\"geometry\", expr(\"ST_SetSRID(ST_GeomFromText(geometry), 4326)\"))\n",
    "\n",
    "start = time.time()\n",
    "# test.write.mode(\"overwrite\").csv(\"./df_joined_csv\")\n",
    "end = time.time()\n",
    "\n",
    "if((end-start)/60 < 1):\n",
    "    print(f\"{end-start:.2f} sec\")\n",
    "else:\n",
    "    print(f\"{(end-start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e4afc-43c2-4155-9748-a4889585ea2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## SpatialRDD JOIN + result to DF\n",
    "\n",
    "from sedona.core.enums.grid_type import GridType\n",
    "from sedona.core.enums.index_type import IndexType\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "join_result = JoinQuery.SpatialJoinQueryFlat(\n",
    "    reg_rdd, census_rdd, useIndex=True, considerBoundaryIntersection=True\n",
    ")\n",
    "\n",
    "join_rdd_df = Adapter.toDf(join_result, reg_rdd.fieldNames, census_rdd.fieldNames, spark)\n",
    "\n",
    "start = time.time()\n",
    "print(join_rdd_df.count())\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{(start-end)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e58831-8952-4360-a3f6-ba04c03f1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_join_df = reg_df.alias(\"reg\").join(prov_df.alias(\"prov\"), expr(\"reg.COD_REG == prov.COD_REG\"), \"inner\")\n",
    "\n",
    "polygon_df = joined_df.alias(\"reg\").filter(expr(\"ST_GeometryType(reg.geometry) = 'Polygon'\"))\n",
    "\n",
    "polygon_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
