{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from contextlib import contextmanager\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import json\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame, functions as F\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, Layout\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from typing import Optional\n",
    "\n",
    "# from IPython.display import *\n",
    "\n",
    "\n",
    "\n",
    "class Enricher:\n",
    "\n",
    "    def __init__(self, crs=\"EPSG:3035\"):\n",
    "        self.crs = crs if type(crs) is not int else f\"EPSG:{crs}\"\n",
    "        self.cores = None\n",
    "        self.res = None\n",
    "        self.sedona = None\n",
    "        self.res_agr = None\n",
    "        self.df1 = None\n",
    "        self.df2 = None\n",
    "        self.dfs_list = {}\n",
    "        self.data_dir = None\n",
    "\n",
    "            \n",
    "    def setup_cluster(self, which=\"wherobots\", ex_mem=26, dr_mem=24, log_level=None):\n",
    "        if which == \"wherobots\":\n",
    "            config = SedonaContext.builder(). \\\n",
    "                config(\"spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\",\n",
    "                    \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "                getOrCreate()\n",
    "\n",
    "            sedona = SedonaContext.create(config)\n",
    "            self.sedona = SedonaContext.create(config)\n",
    "            \n",
    "            self.cores = self.sedona.sparkContext.defaultParallelism\n",
    "            print(f\"Wherobots setup started with {self.cores} cores for parellelism.\")\n",
    "        elif which == \"sedona\":\n",
    "            # config = SedonaContext.builder() .\\\n",
    "            #     config(\"spark.executor.memory\", f\"{ex_mem}g\").\\\n",
    "            #     config(\"spark.driver.memory\", f\"{dr_mem}g\").\\\n",
    "            #     config('spark.jars.packages',\\\n",
    "            #         'org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.0,'\\\n",
    "            #         'org.datasyslab:geotools-wrapper:1.7.0-28.5').\\\n",
    "            #     getOrCreate()\n",
    "\n",
    "            config = SedonaContext.builder() \\\n",
    "                .config(\"spark.executor.memory\", f\"{ex_mem}g\") \\\n",
    "                .config(\"spark.driver.memory\", f\"{dr_mem}g\") \\\n",
    "                .config(\"spark.local.dir\", \"./tmp_spark_spills\") \\\n",
    "                .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "                .config('spark.jars.packages',\n",
    "                    'org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.0,'\n",
    "                    'org.datasyslab:geotools-wrapper:1.7.0-28.5') \\\n",
    "                .getOrCreate()\n",
    "\n",
    "\n",
    "            self.sedona = SedonaContext.create(config)\n",
    "            \n",
    "            if log_level in [\"OFF\", \"ERROR\", \"WARN\", \"INFO\", \"DEBUG\"]:\n",
    "                self.sedona.sparkContext.setLogLevel(log_level)\n",
    "                \n",
    "            self.cores = self.sedona.sparkContext.defaultParallelism\n",
    "            print(f\"Sedona initialized with {self.cores} cores for parellelism.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid 'which'. Choose either 'wherobots' or 'sedona'\")\n",
    "        \n",
    "    @contextmanager\n",
    "    def get_time(self, task_name):\n",
    "        start = time.time()\n",
    "        yield\n",
    "        elapsed = time.time() - start\n",
    "    \n",
    "        print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "              if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "    \n",
    "    def load(self, datasets, data_dir=\"./data\", silent=True):\n",
    "        if data_dir != \"/\" and data_dir.endswith('/'):\n",
    "            data_dir = data_dir.rstrip('/')\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        print(\"\\nLoading datasets...\")\n",
    "        print(f\"Make sure the geometry column is named \\\"geometry\\\" in the datasets\")\n",
    "    \n",
    "        self.datasets = {}\n",
    "        for name, (path, fformat) in datasets.items():\n",
    "            if fformat == \"geoparquet\":\n",
    "                gdf = gpd.read_parquet(path)\n",
    "                crs = f\"EPSG:{gdf.crs.to_epsg()}\"\n",
    "                self.dfs_list[name] = self.sedona.read.format(fformat).load(path)\n",
    "                print(f\"Loaded '{name}': {gdf.shape}, '{crs}'\")\n",
    "            elif fformat == \"geopackage\":\n",
    "                layers = fiona.listlayers(path)\n",
    "                if layers:\n",
    "                    self.dfs_list[name] = self.sedona.read.format(fformat).option(\"tableName\", layers[0]).load(path)\n",
    "                    gdf = gpd.read_file(f'{path}', engine='pyogrio', use_arrow=True)\n",
    "                    crs = gdf.crs\n",
    "                    print(f\"Loaded '{name}': {gdf.shape}, '{crs}'\")\n",
    "                else:\n",
    "                    print(f\"No layers found in GeoPackage '{name}'\")\n",
    "            else:\n",
    "                gdf = gpd.read_file(path)\n",
    "                crs = gdf.crs\n",
    "                self.dfs_list[name] = self.sedona.read.format(fformat).load(path)\n",
    "                print(f\"Loaded '{name}': {gdf.shape}, '{crs}'\")\n",
    "            \n",
    "            self.datasets[name] = (path, fformat, crs)\n",
    "\n",
    "            \n",
    "        print(f\"{len(self.dfs_list)} datasets loaded. \\n\")\n",
    "        \n",
    "        if not silent:\n",
    "            for name, df in self.dfs_list.items():\n",
    "                print(f\"\\n Dataset: \\\"{name}\\\", count: {df.count()}\")\n",
    "    \n",
    "                geometry_types = df.select(F.expr(\"ST_GeometryType(geometry)\")).distinct().collect()\n",
    "                \n",
    "                res_string = [\n",
    "                    f\"{geometry_type} ({(df.filter(F.expr(f\"ST_GeometryType(geometry) = \\'{geometry_type}\\'\")).count() / df.count()) * 100:.2f}%)\"\n",
    "                    for row in geometry_types if (geometry_type := row[0])\n",
    "                ]\n",
    "\n",
    "                # Below for python < 3.12:\n",
    "                # res_string = [\n",
    "                #     f\"{row[0]} ({(df.filter(F.expr(f'ST_GeometryType(geometry) = {row[0]}')).count() / df.count()) * 100:.2f}%)\"\n",
    "                #     for row in geometry_types if row[0]\n",
    "                # ]\n",
    "\n",
    "                print(f\"\\\"{name}\\\" has geometries of type(s): {', '.join(res_string)}\")\n",
    "                df.printSchema()\n",
    "\n",
    "    def force_repartition(self, skip=[]):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            if name not in skip:\n",
    "                self.dfs_list[name] = df.repartition(self.cores)\n",
    "    \n",
    "    def inspect_partitions(self):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            print(f\"'{name}' partitions: {df.rdd.getNumPartitions()}\")\n",
    "            if df.rdd.getNumPartitions() > 1:\n",
    "                print(f\"'{name}' distribution: {df.rdd.glom().map(len).collect()} \\n\")\n",
    "\n",
    "    \n",
    "    def transform_CRS(self, target=None, lazy=True):\n",
    "        if target is None:\n",
    "            target = self.crs\n",
    "        elif type(target) is int:\n",
    "            self.crs = f\"EPSG:{target}\"\n",
    "            target = self.crs\n",
    "        else:\n",
    "            self.crs = target\n",
    "        \n",
    "        print()\n",
    "        print(\"Transforming CRS...\")\n",
    "        for name, df in self.dfs_list.items():\n",
    "            df = df.withColumn(\"geometry\", F.expr(f\"ST_Transform(geometry, '{self.datasets[name][2]}', '{target}')\"))\n",
    "            print(f\"Changed CRS of '{name}': '{self.datasets[name][2]}' to '{target}'\")\n",
    "            self.dfs_list[name] = df\n",
    "            \n",
    "        if not lazy:\n",
    "            self._make_cache(self.dfs_list.values())\n",
    "            \n",
    "    def fix_geometries(self, skip=[]):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            if name not in skip:\n",
    "                invalid_count = df.filter(F.expr(\"NOT ST_IsValid(geometry)\")).count()\n",
    "                print(f\"'{name}' has {((invalid_count / df.count()) * 100 if df.count() > 0 else 0):.2f}% invalid geometries.\")\n",
    "                \n",
    "                if invalid_count > 0:\n",
    "                    df = df.withColumn(\"geometry\", F.expr(\"ST_MakeValid(geometry)\"))\n",
    "                    print(f\"Fixed {invalid_count} geometries in '{name}'\")\n",
    "                else:\n",
    "                    print(f\"Nothing to fix in '{name}'\")\n",
    "                \n",
    "                self.dfs_list[name] = df\n",
    "\n",
    "    \n",
    "    def _make_cache(self, dfs=[]):\n",
    "        for df in dfs:\n",
    "            if isinstance(df, DataFrame):\n",
    "                if df.storageLevel != StorageLevel.NONE:\n",
    "                    df.unpersist()\n",
    "                df.cache()\n",
    "            print(f\"Dataset cached. {df.count()} rows.\")\n",
    "    \n",
    "    def parquet_all(self, preserve_partitions=True):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            print(f\"Saving '{name}' as Parquet...\")\n",
    "            if preserve_partitions:\n",
    "                df.write.mode(\"overwrite\").format(\"geoparquet\").save(f\"{self.data_dir}/pickle_parquets/dfs_list/{name}\")\n",
    "            else:\n",
    "                df.coalesce(1).write.mode(\"overwrite\").format(\"geoparquet\").save(f\"{self.data_dir}/pickle_parquets/dfs_list/{name}\")\n",
    "            print(f\"Saved to '{self.data_dir}/pickle_parquets/dfs_list/{name}'\")\n",
    "\n",
    "    def parquet_this(self, name, df, preserve_partitions=True):\n",
    "        print(f\"Saving dataframe '{name}' as Parquet...\")\n",
    "        if preserve_partitions:\n",
    "            df.write.mode(\"overwrite\").format(\"geoparquet\").save(f\"{self.data_dir}/pickle_parquets/others/{name}\")\n",
    "        else:\n",
    "            df.coalesce(1).write.mode(\"overwrite\").format(\"geoparquet\").save(f\"{self.data_dir}/pickle_parquets/others/{name}\")\n",
    "        print(f\"Saved to '{self.data_dir}/pickle_parquets/others/{name}'\")\n",
    "\n",
    "    def load_from_parquets(self, datasets, parquet_dir: Optional[str]=None):\n",
    "        if parquet_dir is None:\n",
    "            self.parquet_dir = f\"{self.data_dir}/pickle_parquets/dfs_list\"\n",
    "        else:\n",
    "            if parquet_dir != \"/\" and parquet_dir.endswith('/'):\n",
    "                parquet_dir = parquet_dir.rstrip('/')\n",
    "            self.parquet_dir = parquet_dir\n",
    "        \n",
    "        self.datasets = {}\n",
    "\n",
    "        for name, (path, fformat) in datasets.items():\n",
    "            self.datasets[name] = (path, fformat)\n",
    "            print(f\"Loading '{name}' from Parquet...\")\n",
    "            self.dfs_list[name] = self.sedona.read.parquet(f\"{self.parquet_dir}/{name}\")\n",
    "            print(f\"Loaded dataframe '{name}'\")\n",
    "\n",
    "    def load_from_parquet(self, name, parquet_dir: Optional[str]=None):\n",
    "        if parquet_dir is None:\n",
    "            self.parquet_dir = f\"{self.data_dir}/pickle_parquets/others\"\n",
    "        else:\n",
    "            if parquet_dir != \"/\" and parquet_dir.endswth('/'):\n",
    "                parquet_dir = parquet_dir.rstrip('/')\n",
    "            self.parquet_dir = parquet_dir        \n",
    "        \n",
    "        path = f\"{self.parquet_dir}/{name}\"\n",
    "        self.datasets[name] = (path, \"geoparquet\")\n",
    "        print(f\"Loading dataframe '{name}' from Parquet...\")\n",
    "        df = self.sedona.read.parquet(path)\n",
    "        self.dfs_list[name] = df\n",
    "        print(f\"Loaded dataframe '{name}'\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def pickle_this(self, df, name, preserve_partitions=True):\n",
    "        print(f\"Pickling '{name}'...\")\n",
    "        if preserve_partitions:\n",
    "            df.rdd.saveAsPickleFile(f\"/pickles/{name}\")\n",
    "        else:\n",
    "            df.coalesce(1).rdd.saveAsPickleFile(f\"/pickles/{name}\")\n",
    "        print(f\"Saved to './pickles/{name}'\")\n",
    "\n",
    "    def pickle_all(self, preserve_partitions=True):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            path = f\"./pickles/dfs_list/{name}\"\n",
    "            \n",
    "            print(f\"Pickling '{name}'...\")\n",
    "            schema_json = df.schema.json()\n",
    "            with open(f\"{path}_schema.json\", \"w\") as f:\n",
    "                f.write(schema_json)\n",
    "            if preserve_partitions:\n",
    "                df.rdd.saveAsPickleFile(path)\n",
    "            else:\n",
    "                df.coalesce(1).rdd.saveAsPickleFile(path)\n",
    "            print(f\"Saved to {path}\")\n",
    "        \n",
    "    def load_from_pickle(self, name):\n",
    "        pickleRdd = self.sedona.sparkContext.pickleFile(f\"./pickles/{name}\")\n",
    "        return self.sedona.createDataFrame(pickleRdd)\n",
    "    \n",
    "    def load_from_pickles(self, datasets):\n",
    "        self.datasets = {}\n",
    "        for name, (path, fformat) in datasets.items():\n",
    "            self.datasets[name] = (path, fformat)\n",
    "        \n",
    "        for name, _ in self.datasets.items():\n",
    "            with open(f\"./pickles/dfs_list/{name}_schema.json\", \"r\") as f:\n",
    "                schema_json = f.read()\n",
    "            schema = StructType.fromJson(json.loads(schema_json))\n",
    "            pickleRdd = self.sedona.sparkContext.pickleFile(f\"./pickles/dfs_list/{name}\")\n",
    "            self.dfs_list[name] = self.sedona.createDataFrame(pickleRdd)\n",
    "        \n",
    "    def clear_memory(self, *keep):\n",
    "        if self.res.storageLevel != StorageLevel.NONE:\n",
    "            self.res.unpersist()\n",
    "        self.res = None\n",
    "\n",
    "    def join_chey_simple(self, selected_aggs, df1_name, df2_name):\n",
    "        self.res_agr = self.dfs_list[df1_name].alias(\"df1\").join(\n",
    "            self.dfs_list[df2_name].alias(\"df2\"), F.expr(\"ST_Intersects(df1.geometry, df2.geometry)\")\n",
    "        ).select(\n",
    "            F.expr(\"df1.geometry\").alias(\"df1_geom\"),\n",
    "            F.expr(\"df2.geometry\").alias(\"df2_geom\"),\n",
    "            *[f\"df1.{c}\" for c in self.dfs_list[df1_name].columns if c != \"geometry\"],\n",
    "            *[f\"df2.{c}\" for c in self.dfs_list[df2_name].columns if c != \"geometry\" and c not in self.dfs_list[df1_name].columns]\n",
    "        )\n",
    "    \n",
    "    def join_chey_new(self, selected_aggs, df1_name, df2_name, group_by=None, pred=\"ST_Intersects\", rel_str=\"2********\", make_geom=True, ratio=True, madre=False, cache=True, grid_area=None):\n",
    "        \n",
    "        # temp_df1 = self.dfs_list[df1_name]\n",
    "        # temp_df2 = self.dfs_list[df2_name]\n",
    "    \n",
    "        join_expr = f\"{pred}(df1.geometry, df2.geometry)\"\n",
    "        if pred == \"ST_Relate\":\n",
    "            join_expr = f\"{pred}(df1.geometry, df2.geometry, '{rel_str}')\"\n",
    "    \n",
    "        self.res = self.dfs_list[df1_name].alias(\"df1\").join(\n",
    "            self.dfs_list[df2_name].alias(\"df2\"), F.expr(join_expr)\n",
    "        ).select(\n",
    "            F.expr(\"df1.geometry\").alias(\"df1_geom\"),\n",
    "            F.expr(\"df2.geometry\").alias(\"df2_geom\"),\n",
    "            *[f\"df1.{c}\" for c in self.dfs_list[df1_name].columns if c != \"geometry\"],\n",
    "            *[f\"df2.{c}\" for c in self.dfs_list[df2_name].columns if c != \"geometry\" and c not in self.dfs_list[df1_name].columns]\n",
    "        )\n",
    "        \n",
    "        self.res = self.res.withColumn(\"intr_geometry\", F.expr(\"ST_Intersection(df1_geom, df2_geom)\"))\n",
    "        if ratio:\n",
    "            if grid_area > 0:\n",
    "                self.res = self.res.withColumn(\"intr_ratio\", F.expr(f\"ST_Area(intr_geometry) / {grid_area}\"))\n",
    "            else:\n",
    "                self.res = self.res.withColumn(\"intr_ratio\", F.expr(\"ST_Area(intr_geometry) / ST_Area(df2_geom)\"))\n",
    "\n",
    "            agg_exprs = []\n",
    "            for col_name, agg_func in selected_aggs.items():\n",
    "                if agg_func == \"sum\":\n",
    "                    agg_exprs.append(F.sum(F.col(col_name) * F.col(\"intr_ratio\")).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"mean\":\n",
    "                    agg_exprs.append(F.mean(F.col(col_name) * F.col(\"intr_ratio\")).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"min\":\n",
    "                    agg_exprs.append(F.min(F.col(col_name) * F.col(\"intr_ratio\")).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"max\":\n",
    "                    agg_exprs.append(F.max(F.col(col_name) * F.col(\"intr_ratio\")).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"count\":\n",
    "                    agg_exprs.append(F.count(F.col(col_name)).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"first\":\n",
    "                    agg_exprs.append(F.first(F.col(col_name)).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported aggregation function: {agg_func}\")\n",
    "                \n",
    "            df1_cols = [F.first(F.col(f\"df1.{c}\")).alias(c) for c in self.dfs_list[df1_name].columns if c != group_by and c != \"geometry\"]\n",
    "            df1_cols.append(F.first(F.col(\"df1_geom\")).alias(\"geometry\"))\n",
    "            self.res_agr = self.res.groupBy(group_by).agg(*df1_cols, *agg_exprs)\n",
    "\n",
    "            print(f\"Aggregation will result in {self.res_agr.count()} rows.\")\n",
    "        \n",
    "        if madre:\n",
    "            print(\"Preserving overlapping geometries to 'res'\")\n",
    "            columns_to_drop = [\"df1_geom\", \"df2_geom\"] + list(selected_aggs.keys())\n",
    "            self.res = self.res.drop(*columns_to_drop)\n",
    "            self.res = self.res.join(self.res_agr.drop(\"geometry\"), on=group_by, how=\"left\")\n",
    "            self.res = self.res.withColumnRenamed(\"intr_geometry\", \"geometry\")\n",
    "            \n",
    "            def drop_duplicate_columns(df):\n",
    "                unique_cols = []\n",
    "                seen = set()\n",
    "                for idx, name in enumerate(df.columns):\n",
    "                    if name.lower() not in seen:\n",
    "                        unique_cols.append((idx, name))\n",
    "                        seen.add(name.lower())\n",
    "\n",
    "                temp_names = [f\"{i}\" for i in range(len(df.columns))]\n",
    "                df_temp = df.toDF(*temp_names)\n",
    "\n",
    "                select_expr = [F.col(f\"{idx}\").alias(name) for idx, name in unique_cols]\n",
    "                return df_temp.select(*select_expr)\n",
    "\n",
    "            self.res = drop_duplicate_columns(self.res)\n",
    "    \n",
    "        if cache:\n",
    "            if madre:\n",
    "                self._make_cache([self.res, self.res_agr])\n",
    "            else:\n",
    "                self._make_cache(self.res_agr)\n",
    "            \n",
    "        return self.res_agr\n",
    "        \n",
    "\n",
    "    def export(self, df=\"default\", path=\"outputs\", name=\"unnamed\", how=\"repartition\", num=None, clear=False):\n",
    "        if num is None:\n",
    "            num = self.cores\n",
    "        if how == \"repartition\":\n",
    "            self.res = self.res.repartition(num)\n",
    "        elif how == \"coalesce\":\n",
    "            self.res = self.res.coalesce(num)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid 'how'. Choose either 'repartition' or 'coalesce'\")\n",
    "\n",
    "        if df == \"default\":\n",
    "            self.res_agr.write.mode(\"overwrite\").format(\"geoparquet\").save(f\"./{path}/\" + f\"/{name}_agr\")\n",
    "        else:\n",
    "            self.res.write.mode(\"overwrite\").format(\"geoparquet\").save(f\"./{path}/\" + f\"/{name}_madre\")\n",
    "        \n",
    "        if clear:\n",
    "            self.clear_memory()\n",
    "\n",
    "\n",
    "\n",
    "class EnricherUI:\n",
    "    def __init__(self, enricher):\n",
    "        self.enricher = enricher\n",
    "        self.loaded_dataframes = {}\n",
    "        self.selected_cols = []\n",
    "        self.group_by_col = None\n",
    "        self._init_ui()\n",
    "        self.loaded_dataframes = self.list_dataframes_in_memory()\n",
    "        self.df1_dropdown.options = list(self.loaded_dataframes.keys())\n",
    "        self.df2_dropdown.options = list(self.loaded_dataframes.keys())\n",
    "        self.df1_dropdown.disabled = False\n",
    "        self.df2_dropdown.disabled = False        \n",
    "        self.selected_aggs = {}\n",
    "        self.agg_options = [\"sum\", \"count\", \"mean\", \"min\", \"max\", \"first\"]\n",
    "\n",
    "    def list_dataframes_in_memory(self):\n",
    "        # return {name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)}\n",
    "        return {name: df for name, df in self.enricher.dfs_list.items()}\n",
    "    \n",
    "    def _init_ui(self):\n",
    "        # Heading:\n",
    "        self.heading = widgets.HTML(value=\"<h1>Enrich with Overlay & Aggregation</h1>\")\n",
    "\n",
    "        # First line: Enrich <df1> with <df2>\n",
    "        self.df1_dropdown = widgets.Dropdown(options=[], description=\"df1:\", disabled=True, style={'description_width': 'initial'}, layout=widgets.Layout(margin=\"5px 20px\", width=\"150px\"))\n",
    "        self.df2_dropdown = widgets.Dropdown(options=[], description=\"df2:\", disabled=True, style={'description_width': 'initial'}, layout=widgets.Layout(margin=\"5px 20px\", width=\"150px\"))\n",
    "        self.load_status = widgets.HTML(value=\"<small>Status: No dataframes loaded.</small>\")\n",
    "        self.load_button = widgets.Button(description=\"Load\", disabled=True, layout=widgets.Layout(margin=\"5px 0px\", width=\"100px\", border=\"2px solid black\"))\n",
    "        self.load_button.style.font_weight = 'bold'\n",
    "\n",
    "        # Second line: with attributes: <cols>\n",
    "        self.cols_dropdown = widgets.SelectMultiple(options=[], description=\"aggr cols:\", disabled=True, style={'description_width': 'initial'})\n",
    "        self.agg_table_output = widgets.Output()        \n",
    "        \n",
    "        # Third line: unique id: <col>\n",
    "        self.group_by_dropdown = widgets.Dropdown(options=[], description=\"unique id:\", disabled=True, style={'description_width': 'initial'}, layout=widgets.Layout(margin=\"5px 20px\", width=\"250px\"))\n",
    "        \n",
    "        self.agg_status = widgets.HTML(value=\"\")\n",
    "\n",
    "        # Advanced options\n",
    "        self.advanced_checkbox = widgets.Checkbox(value=False, description=\"Advanced options\", style={'description_width': 'initial'})\n",
    "        self.preserve_geoms_checkbox = widgets.Checkbox(value=False, description=\"Preserve overlapping geometries\", disabled=False)\n",
    "        self.intersection_ratio_checkbox = widgets.Checkbox(value=False, description=\"Enricher has uniform grids\", disabled=False)\n",
    "        self.grid_area_text = widgets.FloatText(value=1e6, description=\"Grid area:\", disabled=True, layout=widgets.Layout(width=\"165px\"))\n",
    "        self.custom_predicate_checkbox = widgets.Checkbox(value=False, description=\"Custom ST_Relate predicate string:\", disabled=False, layout=Layout(width=\"500px\"))\n",
    "        self.custom_predicate_text = widgets.Text(value=\"2********\", disabled=True, layout=widgets.Layout(width=\"200px\"))\n",
    "        \n",
    "        self.go_button = widgets.Button(description=\"Go\", disabled=True, layout=widgets.Layout(margin=\"5px 0px\", width=\"100px\", border=\"2px solid black\"))\n",
    "        self.go_button.style.font_weight = 'bold'\n",
    "        \n",
    "        # Console output text box\n",
    "        self.console_output = widgets.Output(layout=widgets.Layout(width=\"100%\", height=\"200px\", border=\"1px solid black\"))\n",
    "        self.clear_console_button = widgets.Button(description=\"Clear\", layout=widgets.Layout(width='80px', border=\"2px solid black\"))\n",
    "        self.clear_console_button.style.font_weight = 'bold'\n",
    "\n",
    "        # Layout\n",
    "        self._setup_layout()\n",
    "        self._setup_event_handlers()\n",
    "\n",
    "    def _setup_layout(self):\n",
    "        # First line: Enrich <df1> with <df2>\n",
    "        df_selection_line = widgets.HBox([\n",
    "            widgets.HTML(value=\"<h2 style='display: inline; margin-right: 10px;'>Enrich </h2>\"),\n",
    "            self.df1_dropdown,\n",
    "            widgets.HTML(value=\"<h2 style='display: inline; margin-right: 10px;'> with </h2>\"),\n",
    "            self.df2_dropdown,\n",
    "            self.load_button\n",
    "        ])\n",
    "\n",
    "        # Second line: with attributes: <cols>\n",
    "        cols_selection_line = widgets.HBox([\n",
    "            widgets.HTML(value=\"<h2 style='display: inline; margin-right: 10px;'> with attributes: </h2>\"),\n",
    "            self.cols_dropdown,\n",
    "            self.agg_table_output,\n",
    "        ])\n",
    "\n",
    "        # Third line: unique id: <col>\n",
    "        grp_by_selection_line = widgets.HBox([\n",
    "            widgets.HTML(value=f\"<h2 style='display: inline; margin-right: 10px;'><span id='unique_id_text'>unique identifier:</span> </h2>\"),\n",
    "            self.group_by_dropdown,\n",
    "            self.go_button\n",
    "        ])\n",
    "\n",
    "        # Advanced options\n",
    "        self.advanced_options = widgets.VBox(\n",
    "            [\n",
    "                self.preserve_geoms_checkbox,\n",
    "                widgets.HBox([self.intersection_ratio_checkbox, self.grid_area_text]),\n",
    "                widgets.HBox([self.custom_predicate_checkbox, self.custom_predicate_text]),\n",
    "            ],\n",
    "            layout=widgets.Layout(max_width=\"600px\", display=\"none\")\n",
    "        )\n",
    "\n",
    "        # Main layout\n",
    "        self.main_layout = widgets.VBox([\n",
    "            self.heading,\n",
    "            widgets.HTML(value=\"<div style='height: 5px;'></div>\"),\n",
    "            df_selection_line,\n",
    "            self.load_status,\n",
    "            cols_selection_line,\n",
    "            widgets.HTML(value=\"<div style='height: 4px;'></div>\"),\n",
    "            grp_by_selection_line,\n",
    "            self.agg_status,\n",
    "            widgets.HTML(value=\"<div style='height: 3px;'></div>\"),\n",
    "            self.advanced_checkbox,\n",
    "            self.advanced_options,\n",
    "            widgets.HBox([self.console_output, self.clear_console_button])\n",
    "        ])\n",
    "\n",
    "        # Display everything\n",
    "        display(self.main_layout)\n",
    "\n",
    "    def _setup_event_handlers(self):\n",
    "        # Enable/disable load button based on dataframe selection\n",
    "        def on_df_selection_change(change):\n",
    "            if self.df1_dropdown.value and self.df2_dropdown.value:\n",
    "                self.load_button.disabled = False\n",
    "            else:\n",
    "                self.load_button.disabled = True\n",
    "        self.df1_dropdown.observe(on_df_selection_change, names='value')\n",
    "        self.df2_dropdown.observe(on_df_selection_change, names='value')\n",
    "\n",
    "        # Handle load button click\n",
    "        def on_load_button_click(b):\n",
    "            try:\n",
    "                df1_name = self.df1_dropdown.value\n",
    "                df2_name = self.df2_dropdown.value\n",
    "\n",
    "                if df1_name not in self.loaded_dataframes or df2_name not in self.loaded_dataframes:\n",
    "                    raise ValueError(\"Selected dataframes are not loaded in memory.\")\n",
    "\n",
    "                # Set the selected dataframes in the Enricher\n",
    "                self.enricher.df1 = self.loaded_dataframes[df1_name]\n",
    "                self.enricher.df2 = self.loaded_dataframes[df2_name]\n",
    "\n",
    "                # Update column dropdowns\n",
    "                self.cols_dropdown.options = self.loaded_dataframes[self.df2_dropdown.value].columns\n",
    "                self.group_by_dropdown.options = self.loaded_dataframes[self.df1_dropdown.value].columns\n",
    "                self.cols_dropdown.disabled = False\n",
    "                self.group_by_dropdown.disabled = False\n",
    "\n",
    "                self.load_status.value = f\"<small>Status: Loaded {df1_name} and {df2_name}.</small>\"\n",
    "                self.main_layout.children[6].children[0].value = f\"<h2 style='display: inline; margin-right: 10px;'>{df1_name}'s unique identifier: </h2>\"\n",
    "            except Exception as e:\n",
    "                self.load_status.value = f\"<small>Error: {str(e)}</small>\"\n",
    "\n",
    "        self.load_button.on_click(on_load_button_click)\n",
    "\n",
    "        # Handle column selection\n",
    "        def on_cols_change(change):\n",
    "            for col in change[\"new\"]:\n",
    "                if col not in self.selected_cols:\n",
    "                    self.selected_cols.append(col)\n",
    "            \n",
    "            self.cols_dropdown.options = [col for col in self.loaded_dataframes[self.df2_dropdown.value].columns if col not in self.selected_cols]\n",
    "            \n",
    "            # Preserve previously selected operations, default to \"sum\" for new columns\n",
    "            for col in self.selected_cols:\n",
    "                if col not in self.selected_aggs:\n",
    "                    self.selected_aggs[col] = \"sum\"            \n",
    "            \n",
    "            def generate_agg_table():\n",
    "                headers = [\"Selected Column\", \"Operation\", \"\"]\n",
    "                \n",
    "                cell_style = widgets.Layout(\n",
    "                    # border=\"1px solid black\", \n",
    "                    padding=\"0px 2px\",\n",
    "                    align_items=\"center\", \n",
    "                    justify_content=\"center\", \n",
    "                    width=\"125px\"\n",
    "                )\n",
    "                \n",
    "                clr_style = widgets.Layout(border=\"1px solid black\", padding=\"0px 2px\",align_items=\"center\", justify_content=\"center\", width=\"80px\")\n",
    "                \n",
    "                header_row = [\n",
    "                    widgets.HTML(f\"<b>{headers[0]}</b>\", layout=cell_style),\n",
    "                    widgets.HTML(f\"<b>{headers[1]}</b>\", layout=cell_style),\n",
    "                    widgets.HTML(\"\", layout=widgets.Layout(padding=\"0px 2px\",align_items=\"center\", justify_content=\"center\", width=\"80px\"))\n",
    "                ]\n",
    "            \n",
    "                rows = []\n",
    "                for col in self.selected_aggs:\n",
    "                    dropdown = widgets.Dropdown(\n",
    "                        options=self.agg_options, \n",
    "                        value=self.selected_aggs[col], \n",
    "                        layout=cell_style\n",
    "                    )\n",
    "                    dropdown.observe(lambda change, col=col: self.selected_aggs.update({col: change[\"new\"]}), names=\"value\")\n",
    "            \n",
    "                    clear_button = widgets.Button(description=\"Clear\", layout=clr_style)\n",
    "\n",
    "                    def on_clear(btn, col=col):\n",
    "                        self.selected_cols.remove(col)\n",
    "                        self.agg_status.value = f\"Status: Aggregating with: {', '.join([f'<b>{col}</b>' for col in self.selected_cols]) if self.selected_cols else '<i>select cols</i>'}, grouping by: {f'<b>{self.group_by_col}</b>' if self.group_by_col else '<i>select cols</i>'}\"\n",
    "                        del self.selected_aggs[col]\n",
    "                        self.cols_dropdown.options = [col for col in self.loaded_dataframes[self.df2_dropdown.value].columns if col not in self.selected_cols]\n",
    "                        \n",
    "                        with self.agg_table_output:\n",
    "                            self.agg_table_output.clear_output()\n",
    "                            display(generate_agg_table())\n",
    "                        \n",
    "                    clear_button.on_click(on_clear)\n",
    "                        \n",
    "                    rows.extend([\n",
    "                        widgets.HTML(col, layout=cell_style),\n",
    "                        dropdown,\n",
    "                        clear_button\n",
    "                    ])\n",
    "                \n",
    "                scrollable_container = widgets.VBox([\n",
    "                    widgets.GridBox(\n",
    "                        children=header_row + rows,\n",
    "                        layout=widgets.Layout(\n",
    "                            grid_template_columns=\"150px 150px 90px\",\n",
    "                            grid_template_rows=\"auto\",\n",
    "                            padding=\"1px\",\n",
    "                            width=\"max-content\",\n",
    "                        )\n",
    "                    )\n",
    "                ], layout=widgets.Layout(\n",
    "                    max_height=\"150px\",\n",
    "                    overflow_y=\"auto\",\n",
    "                    border=\"1px solid black\"\n",
    "                ))\n",
    "            \n",
    "                return scrollable_container\n",
    "\n",
    "\n",
    "            with self.agg_table_output:\n",
    "                self.agg_table_output.clear_output()\n",
    "                display(generate_agg_table())\n",
    "\n",
    "            self.agg_status.value = f\"Status: Will aggregate with: {', '.join([f'<b>{col}</b>' for col in self.selected_cols]) if self.selected_cols else '<i>select cols</i>'}; grouping by: {f'<b>{self.group_by_col}</b>' if self.group_by_col else '<i>select cols</i>'}\"\n",
    "            if self.group_by_col and self.selected_cols:\n",
    "                self.go_button.disabled = False            \n",
    "        self.cols_dropdown.observe(on_cols_change, names='value')\n",
    "\n",
    "        \n",
    "        def on_group_by_change(change):\n",
    "            self.group_by_col = change[\"new\"]\n",
    "            self.agg_status.value = f\"Status: Will aggregate with: {', '.join([f'<b>{col}</b>' for col in self.selected_cols]) if self.selected_cols else '<i>select col</i>'}; grouping by: {f'<b>{self.group_by_col}</b>' if self.group_by_col else '<i>select col</i>'}\"\n",
    "            if self.group_by_col and self.selected_cols:\n",
    "                self.go_button.disabled = False                \n",
    "                \n",
    "\n",
    "        self.group_by_dropdown.observe(on_group_by_change, names='value')\n",
    "\n",
    "\n",
    "        def on_advanced_checkbox_change(change):\n",
    "            if change[\"new\"]:\n",
    "                self.advanced_options.layout.display = \"block\"                \n",
    "\n",
    "            else:\n",
    "                self.advanced_options.layout.display = \"none\"\n",
    "\n",
    "        self.advanced_checkbox.observe(on_advanced_checkbox_change, names='value')\n",
    "\n",
    "        def on_intersection_ratio_change(change):\n",
    "            self.grid_area_text.disabled = not change[\"new\"]\n",
    "        self.intersection_ratio_checkbox.observe(on_intersection_ratio_change, names='value')\n",
    "\n",
    "        def on_custom_predicate_change(change):\n",
    "            self.custom_predicate_text.disabled = not change[\"new\"]\n",
    "        self.custom_predicate_checkbox.observe(on_custom_predicate_change, names='value')\n",
    "\n",
    "        def on_go_button_click(b):\n",
    "            with self.console_output:\n",
    "                try:\n",
    "                    print(\"Performing operation. This may take a while. Check logs for Spark logs and completion status.\")\n",
    "                    self.enricher.join_chey_new(\n",
    "                        selected_aggs=self.selected_aggs,\n",
    "                        df1_name=self.df1_dropdown.value,\n",
    "                        df2_name=self.df2_dropdown.value,\n",
    "                        group_by=self.group_by_col,\n",
    "                        pred=\"ST_Relate\" if self.custom_predicate_checkbox.value else \"ST_Intersects\",\n",
    "                        rel_str=self.custom_predicate_text.value if self.custom_predicate_checkbox.value else \"2********\",\n",
    "                        make_geom=True,\n",
    "                        ratio=self.intersection_ratio_checkbox.value,\n",
    "                        madre=self.preserve_geoms_checkbox.value,\n",
    "                        cache=True,\n",
    "                        grid_area=float(self.grid_area_text.value) if self.intersection_ratio_checkbox.value else 1e6\n",
    "                    )\n",
    "                    print(\"Enrichment operation completed.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "        \n",
    "        self.go_button.on_click(on_go_button_click)\n",
    "\n",
    "        def on_clear_console_button_click(b):\n",
    "            self.console_output.clear_output()\n",
    "        self.clear_console_button.on_click(on_clear_console_button_click)\n",
    "\n",
    "    def add_dataframe(self, name, dataframe):\n",
    "        self.loaded_dataframes[name] = dataframe\n",
    "        self.df1_dropdown.options = list(self.loaded_dataframes.keys())\n",
    "        self.df2_dropdown.options = list(self.loaded_dataframes.keys())\n",
    "        self.df1_dropdown.disabled = False\n",
    "        self.df2_dropdown.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/21 17:59:29 WARN Utils: Your hostname, marvin resolves to a loopback address: 127.0.1.1; using 172.20.27.4 instead (on interface eth0)\n",
      "25/02/21 17:59:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/21 17:59:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/21 17:59:37 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o32.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo\norg.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m obj \u001b[38;5;241m=\u001b[39m Enricher(crs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:3035\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m, in \u001b[0;36mEnricher.setup_cluster\u001b[0;34m(self, which, ex_mem, dr_mem, log_level)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwherobots\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# config = SedonaContext.builder(). \\\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#     config(\"spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\",\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#         \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m#     getOrCreate()\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     config \u001b[38;5;241m=\u001b[39m SedonaContext\u001b[38;5;241m.\u001b[39mbuilder() \\\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.serializer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.serializer.KryoSerializer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.kryo.registrator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.sedona.core.serde.SedonaKryoRegistrator\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> 49\u001b[0m     sedona \u001b[38;5;241m=\u001b[39m \u001b[43mSedonaContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msedona \u001b[38;5;241m=\u001b[39m SedonaContext\u001b[38;5;241m.\u001b[39mcreate(config)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msedona\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mdefaultParallelism\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/sedona/spark/SedonaContext.py:43\u001b[0m, in \u001b[0;36mSedonaContext.create\u001b[0;34m(cls, spark)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mcls\u001b[39m, spark: SparkSession) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SparkSession:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    This is the core of whole package, It uses py4j to run wrapper which takes existing SparkSession\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    and register the core logics of Apache Sedona, for this SparkSession.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    :return: SedonaContext which is an instance of SparkSession\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT 1 as geom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# with Spark Connect there is no local JVM\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote():\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo\norg.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "obj = Enricher(crs=\"EPSG:3035\")\n",
    "obj.setup_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/21 17:55:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/02/21 17:55:41 ERROR TorrentBroadcast: Store broadcast broadcast_1 fail, remove all pieces of the broadcast\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo\norg.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     19\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountries\u001b[39m\u001b[38;5;124m\"\u001b[39m: (path_contr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapefile\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregions_IT\u001b[39m\u001b[38;5;124m\"\u001b[39m: (path_reg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapefile\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# \"census\": (path_census, \"\"),\u001b[39;00m\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     31\u001b[0m obj \u001b[38;5;241m=\u001b[39m Enricher(crs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:3035\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msedona\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m26\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdr_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mERROR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m obj\u001b[38;5;241m.\u001b[39mload(datasets, data_dir\u001b[38;5;241m=\u001b[39mdata_dir, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m obj\u001b[38;5;241m.\u001b[39mfix_geometries(skip\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpop_grids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpop_grids_new\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m, in \u001b[0;36mEnricher.setup_cluster\u001b[0;34m(self, which, ex_mem, dr_mem, log_level)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m which \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msedona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# config = SedonaContext.builder() .\\\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m#     config(\"spark.executor.memory\", f\"{ex_mem}g\").\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m#         'org.datasyslab:geotools-wrapper:1.7.0-28.5').\\\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#     getOrCreate()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     config \u001b[38;5;241m=\u001b[39m SedonaContext\u001b[38;5;241m.\u001b[39mbuilder() \\\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex_mem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdr_mem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morg.datasyslab:geotools-wrapper:1.7.0-28.5\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msedona \u001b[38;5;241m=\u001b[39m \u001b[43mSedonaContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_level \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOFF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msedona\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(log_level)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/sedona/spark/SedonaContext.py:43\u001b[0m, in \u001b[0;36mSedonaContext.create\u001b[0;34m(cls, spark)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mcls\u001b[39m, spark: SparkSession) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SparkSession:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    This is the core of whole package, It uses py4j to run wrapper which takes existing SparkSession\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    and register the core logics of Apache Sedona, for this SparkSession.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    :return: SedonaContext which is an instance of SparkSession\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT 1 as geom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# with Spark Connect there is no local JVM\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote():\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/benchmark_data/sedona_venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: org.apache.spark.SparkException: Failed to register classes with Kryo\norg.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1595)\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\n\tat org.apache.spark.scheduler.DAGScheduler.handleMapStageSubmitted(DAGScheduler.scala:1377)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3007)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to register classes with Kryo\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:178)\n\tat org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174)\n\tat org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105)\n\tat com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)\n\tat org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112)\n\tat org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:401)\n\tat org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:275)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:487)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:363)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: org.apache.sedona.core.serde.SedonaKryoRegistrator\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:43)\n\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$7(KryoSerializer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:181)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# file paths:\n",
    "\n",
    "path_contr = f\"{data_dir}/data_EU/countries_shp/\"\n",
    "path_reg = f\"{data_dir}/data_Italy/regioni/\"\n",
    "path_prov = f\"{data_dir}/data_Italy/provinci\"\n",
    "path_com_EU = f\"{data_dir}/data_EU/comuni_shp/\"\n",
    "path_com = f\"{data_dir}/data_Italy/comuni/\"\n",
    "path_grids = f\"{data_dir}/data_EU/census_grid_EU/grids_corrected.parquet\"\n",
    "path_grids_new = f\"{data_dir}/data_EU/census_grid_EU/grids_new.gpkg\"\n",
    "\n",
    "\n",
    "# datasets:\n",
    "# format: {display_name: (path, file_format), ...}\n",
    "\n",
    "datasets = {\n",
    "    \"countries\": (path_contr, \"shapefile\"),\n",
    "    \"regions_IT\": (path_reg, \"shapefile\"),\n",
    "    \"provinces_IT\": (path_prov, \"shapefile\"),\n",
    "    \"comuni_EU\": (path_com_EU, \"shapefile\"),\n",
    "    \"comuni_IT\": (path_com, \"shapefile\"),\n",
    "    \"pop_grids\": (path_grids, \"geoparquet\"),\n",
    "    \"pop_grids_new\": (path_grids_new, \"geopackage\")\n",
    "    # \"census\": (path_census, \"\"),\n",
    "}\n",
    "\n",
    "\n",
    "obj = Enricher(crs=\"EPSG:3035\")\n",
    "obj.setup_cluster(which=\"sedona\", ex_mem=26, dr_mem=24, log_level=\"ERROR\")\n",
    "\n",
    "obj.load(datasets, data_dir=data_dir, silent=True)\n",
    "obj.fix_geometries(skip=['pop_grids', 'pop_grids_new'])\n",
    "obj.force_repartition(skip=['pop_grids'])\n",
    "# obj.inspect_partitions()\n",
    "obj.transform_CRS(lazy=False)\n",
    "obj.parquet_all(preserve_partitions=True)\n",
    "\n",
    "\n",
    "# obj.dfs_list['comuni_EU'] = obj.dfs_list['comuni_EU'].filter(F.col('CNTR_ID').isin([\"IT\", \"DE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'countries' from Parquet...\n",
      "Loaded dataframe 'countries'\n",
      "Loading 'regions_IT' from Parquet...\n",
      "Loaded dataframe 'regions_IT'\n",
      "Loading 'provinces_IT' from Parquet...\n",
      "Loaded dataframe 'provinces_IT'\n",
      "Loading 'comuni_EU' from Parquet...\n",
      "Loaded dataframe 'comuni_EU'\n",
      "Loading 'comuni_IT' from Parquet...\n",
      "Loaded dataframe 'comuni_IT'\n",
      "Loading 'pop_grids' from Parquet...\n",
      "Loaded dataframe 'pop_grids'\n",
      "Loading 'pop_grids_new' from Parquet...\n",
      "Loaded dataframe 'pop_grids_new'\n"
     ]
    }
   ],
   "source": [
    "# Load from pickles\n",
    "\n",
    "data_dir = \"./data\"\n",
    "\n",
    "# file paths:\n",
    "\n",
    "path_contr = f\"{data_dir}/data_EU/countries_shp/\"\n",
    "path_reg = f\"{data_dir}/data_Italy/regioni/\"\n",
    "path_prov = f\"{data_dir}/data_Italy/provinci\"\n",
    "path_com_EU = f\"{data_dir}/data_EU/comuni_shp/\"\n",
    "path_com = f\"{data_dir}/data_Italy/comuni/\"\n",
    "path_grids = f\"{data_dir}/data_EU/census_grid_EU/grids_corrected.parquet\"\n",
    "path_grids_new = f\"{data_dir}/data_EU/census_grid_EU/grids_new.gpkg\"\n",
    "\n",
    "\n",
    "# datasets:\n",
    "# format: {display_name: (path, file_format), ...}\n",
    "\n",
    "datasets = {\n",
    "    \"countries\": (path_contr, \"shapefile\"),\n",
    "    \"regions_IT\": (path_reg, \"shapefile\"),\n",
    "    \"provinces_IT\": (path_prov, \"shapefile\"),\n",
    "    \"comuni_EU\": (path_com_EU, \"shapefile\"),\n",
    "    \"comuni_IT\": (path_com, \"shapefile\"),\n",
    "    \"pop_grids\": (path_grids, \"geoparquet\"),\n",
    "    \"pop_grids_new\": (path_grids_new, \"geopackage\")\n",
    "    # \"census\": (path_census, \"\"),\n",
    "}\n",
    "\n",
    "# obj = Enricher(crs=\"EPSG:3035\")\n",
    "# obj.setup_cluster(which=\"sedona\", ex_mem=26, dr_mem=24, log_level=\"ERROR\")\n",
    "obj.load_from_parquets(datasets)\n",
    "# obj.inspect_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMM_ID', 'CNTR_ID', 'CNTR_CODE', 'COMM_NAME', 'NAME_ASCI', 'TRUE_FLAG', 'NSI_CODE', 'NAME_NSI', 'NAME_LATN', 'NUTS_CODE', 'FID', 'DIST_BORD', 'TOT_P_2018', 'TOT_P_2006', 'GRD_ID', 'TOT_P_2011', 'Y_LLC', 'NUTS2016_3', 'NUTS2016_2', 'NUTS2016_1', 'NUTS2016_0', 'LAND_PC', 'X_LLC', 'NUTS2021_3', 'NUTS2021_2', 'NUTS2021_1', 'NUTS2021_0', 'geometry', 'intr_ratio', 'TOT_P_2021_agr_sum', 'DIST_COAST_agr_mean']\n",
      "Saving dataframe 'pop_new_X_comuni_EU_res' as Parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to './pickle_parquets/others/pop_new_X_comuni_EU_res'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# obj.parquet_this(\"pop_new_X_comuni_EU\", obj.res_agr, preserve_partitions=False)\n",
    "\n",
    "obj.parquet_this(\"pop_new_X_comuni_EU_res\", obj.res, preserve_partitions=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20138849a0a4028a69f7f4d345753f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Enrich with Overlay & Aggregation</h1>'), HTML(value=\"<div style='height: 5px;'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GUI\n",
    "\n",
    "obj_ui = EnricherUI(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unique_values = obj.res.select('CNTR_ID').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m shape\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdecimal\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241m.\u001b[39mres_agr\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# temp_df = obj.res.filter(F.col('CNTR_ID').isin(\"NL\", \"BE\", \"DE\", \"IT\"))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# temp_df = obj.dfs_list['comuni_EU']\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# temp_df = obj.dfs_list['pop_grids'].filter(F.col('T')>1000)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprep_for_map\u001b[39m(res_agr, crs, geom_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'obj' is not defined"
     ]
    }
   ],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import decimal\n",
    "\n",
    "temp_df = obj.res_agr\n",
    "# temp_df = obj.res.filter(F.col('CNTR_ID').isin(\"NL\", \"BE\", \"DE\", \"IT\"))\n",
    "# temp_df = obj.dfs_list['comuni_EU']\n",
    "# temp_df = obj.dfs_list['pop_grids'].filter(F.col('T')>1000)\n",
    "\n",
    "\n",
    "def prep_for_map(res_agr, crs, geom_col='geometry'):\n",
    "    df = res_agr.toPandas()\n",
    "    df = df.map(lambda x: float(x) if isinstance(x, decimal.Decimal) else x)\n",
    "    df['geometry'] = df[f'{geom_col}'].apply(lambda geom: shape(geom))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    gdf.crs = crs\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=prep_for_map(temp_df, obj.crs, geom_col=\"geometry\"), name=\"pop_grids\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import decimal\n",
    "\n",
    "# temp_df = obj.res_agr\n",
    "temp_df = obj.dfs_list['pop_grids_new']\n",
    "# temp_df = obj.dfs_list['comuni_EU']\n",
    "# temp_df = obj.dfs_list['pop_grids_new'].filter(F.col('CNTR_ID').isin(\"NL\", \"BE\", \"DE\", \"IT\"))\n",
    "\n",
    "\n",
    "def prep_for_map(res_agr, crs, geom_col='geometry'):\n",
    "    df = res_agr.toPandas()\n",
    "    df = df.map(lambda x: float(x) if isinstance(x, decimal.Decimal) else x)\n",
    "    df['geometry'] = df[f'{geom_col}'].apply(lambda geom: shape(geom))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    gdf.crs = crs\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=prep_for_map(temp_df, obj.crs, geom_col=\"geometry\"), name=\"pop_grids\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_EU_com_enriched = obj.res_agr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a537a4fb6604e8fab471ca6fb390dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'res_agr': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "com_pop = obj.res_agr\n",
    "\n",
    "res_agr = com_pop.toPandas()\n",
    "res_agr['geometry'] = res_agr['geometry'].apply(lambda geom: shape(geom))\n",
    "\n",
    "gdf = gpd.GeoDataFrame(res_agr, geometry='geometry')\n",
    "gdf.crs = \"EPSG:3035\"\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=gdf, name=\"res_agr\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sedona_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
