{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from contextlib import contextmanager\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import json\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import DataFrame, functions as F\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, Layout\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "\n",
    "# from IPython.display import *\n",
    "\n",
    "\n",
    "\n",
    "class Enricher:\n",
    "\n",
    "    def __init__(self, crs=\"EPSG:3035\"):\n",
    "        self.crs = crs if type(crs) is not int else f\"EPSG:{crs}\"\n",
    "        self.cores = None\n",
    "        self.res = None\n",
    "        self.sedona = None\n",
    "        self.res_agr = None\n",
    "        self.df1 = None\n",
    "        self.df2 = None\n",
    "        self.dfs_list = {}\n",
    "\n",
    "            \n",
    "    def setup_cluster(self, which=\"wherobots\", ex_mem=26, dr_mem=24, log_level=None):\n",
    "        if which == \"wherobots\":\n",
    "            config = SedonaContext.builder().getOrCreate()\n",
    "            self.sedona = SedonaContext.create(config)\n",
    "            \n",
    "            self.cores = self.sedona.sparkContext.defaultParallelism\n",
    "            print(f\"Wherobots setup started with {self.cores} cores for parellelism.\")\n",
    "        elif which == \"sedona\":\n",
    "            # config = SedonaContext.builder() .\\\n",
    "            #     config(\"spark.executor.memory\", f\"{ex_mem}g\").\\\n",
    "            #     config(\"spark.driver.memory\", f\"{dr_mem}g\").\\\n",
    "            #     config('spark.jars.packages',\\\n",
    "            #         'org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.0,'\\\n",
    "            #         'org.datasyslab:geotools-wrapper:1.7.0-28.5').\\\n",
    "            #     getOrCreate()\n",
    "\n",
    "            config = SedonaContext.builder() \\\n",
    "                .config(\"spark.executor.memory\", f\"{ex_mem}g\") \\\n",
    "                .config(\"spark.driver.memory\", f\"{dr_mem}g\") \\\n",
    "                .config(\"spark.local.dir\", \"./tmp_spark_spills\") \\\n",
    "                .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "                .config('spark.jars.packages',\n",
    "                    'org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.0,'\n",
    "                    'org.datasyslab:geotools-wrapper:1.7.0-28.5') \\\n",
    "                .getOrCreate()\n",
    "\n",
    "\n",
    "            self.sedona = SedonaContext.create(config)\n",
    "            \n",
    "            if log_level in [\"OFF\", \"ERROR\", \"WARN\", \"INFO\", \"DEBUG\"]:\n",
    "                self.sedona.sparkContext.setLogLevel(log_level)\n",
    "                \n",
    "            self.cores = self.sedona.sparkContext.defaultParallelism\n",
    "            print(f\"Sedona initialized with {self.cores} cores for parellelism.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid 'which'. Choose either 'wherobots' or 'sedona'\")\n",
    "        \n",
    "    @contextmanager\n",
    "    def get_time(self, task_name):\n",
    "        start = time.time()\n",
    "        yield\n",
    "        elapsed = time.time() - start\n",
    "    \n",
    "        print(f\"{task_name}... DONE in {(elapsed/60):.2f} min\" \\\n",
    "              if elapsed >= 60 else f\"{task_name}... DONE in {elapsed:.2f} sec\")\n",
    "\n",
    "    \n",
    "    def load(self, datasets, silent=True):\n",
    "        print(\"\\nLoading datasets...\")\n",
    "        print(f\"Make sure the geometry column is named \\\"geometry\\\" in the datasets\")\n",
    "    \n",
    "        self.datasets = {}\n",
    "        for name, (path, fformat) in datasets.items():\n",
    "            if fformat == \"geoparquet\":\n",
    "                gdf = gpd.read_parquet(path)\n",
    "                crs = f\"EPSG:{gdf.crs.to_epsg()}\"\n",
    "                self.dfs_list[name] = self.sedona.read.format(fformat).load(path)\n",
    "                print(f\"Loaded '{name}': {gdf.shape}, '{crs}'\")\n",
    "            elif fformat == \"geopackage\":\n",
    "                layers = fiona.listlayers(path)\n",
    "                if layers:\n",
    "                    self.dfs_list[name] = self.sedona.read.format(fformat).option(\"tableName\", layers[0]).load(path)\n",
    "                    gdf = gpd.read_file(f'{path}', engine='pyogrio', use_arrow=True)\n",
    "                    crs = gdf.crs\n",
    "                    print(f\"Loaded '{name}': {gdf.shape}, '{crs}'\")\n",
    "                else:\n",
    "                    print(f\"No layers found in GeoPackage '{name}'\")\n",
    "            else:\n",
    "                gdf = gpd.read_file(path)\n",
    "                crs = gdf.crs\n",
    "                self.dfs_list[name] = self.sedona.read.format(fformat).load(path)\n",
    "                print(f\"Loaded '{name}': {gdf.shape}, '{crs}'\")\n",
    "            \n",
    "            self.datasets[name] = (path, fformat, crs)\n",
    "\n",
    "            \n",
    "        print(f\"{len(self.dfs_list)} datasets loaded. \\n\")\n",
    "        \n",
    "        if not silent:\n",
    "            for name, df in self.dfs_list.items():\n",
    "                print(f\"\\n Dataset: \\\"{name}\\\", count: {df.count()}\")\n",
    "    \n",
    "                geometry_types = df.select(F.expr(\"ST_GeometryType(geometry)\")).distinct().collect()\n",
    "                \n",
    "                res_string = [\n",
    "                    f\"{geometry_type} ({(df.filter(F.expr(f'ST_GeometryType(geometry) = \\'{geometry_type}\\'')).count() / df.count()) * 100:.2f}%)\"\n",
    "                    for row in geometry_types if (geometry_type := row[0])\n",
    "                ]\n",
    "                \n",
    "                print(f\"\\\"{name}\\\" has geometries of type(s): {', '.join(res_string)}\")\n",
    "                df.printSchema()\n",
    "\n",
    "    def force_repartition(self, skip=[]):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            if name not in skip:\n",
    "                self.dfs_list[name] = df.repartition(self.cores)\n",
    "    \n",
    "    def inspect_partitions(self):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            print(f\"'{name}' partitions: {df.rdd.getNumPartitions()}\")\n",
    "            if df.rdd.getNumPartitions() > 1:\n",
    "                print(f\"'{name}' distribution: {df.rdd.glom().map(len).collect()} \\n\")\n",
    "\n",
    "    \n",
    "    def transform(self, target=None, lazy=True):\n",
    "        if target is None:\n",
    "            target = self.crs\n",
    "        elif type(target) is int:\n",
    "            self.crs = f\"EPSG:{target}\"\n",
    "            target = self.crs\n",
    "        else:\n",
    "            self.crs = target\n",
    "        \n",
    "        print()\n",
    "        print(\"Transforming CRS...\")\n",
    "        for name, df in self.dfs_list.items():\n",
    "            df = df.withColumn(\"geometry\", F.expr(f\"ST_Transform(geometry, '{self.datasets[name][2]}', '{target}')\"))\n",
    "            print(f\"Changed CRS of '{name}': '{self.datasets[name][2]}' to '{target}'\")\n",
    "            self.dfs_list[name] = df\n",
    "            \n",
    "        if not lazy:\n",
    "            self._make_cache(self.dfs_list.values())\n",
    "            \n",
    "    def fix_geometries(self, skip=[]):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            if name not in skip:\n",
    "                invalid_count = df.filter(F.expr(\"NOT ST_IsValid(geometry)\")).count()\n",
    "                print(f\"'{name}' has {((invalid_count / df.count()) * 100 if df.count() > 0 else 0):.2f}% invalid geometries.\")\n",
    "                \n",
    "                if invalid_count > 0:\n",
    "                    df = df.withColumn(\"geometry\", F.expr(\"ST_MakeValid(geometry)\"))\n",
    "                    print(f\"Fixed {invalid_count} geometries in '{name}'\")\n",
    "                else:\n",
    "                    print(f\"Nothing to fix in '{name}'\")\n",
    "                \n",
    "                self.dfs_list[name] = df\n",
    "\n",
    "    \n",
    "    def _make_cache(self, dfs=[]):\n",
    "        for df in dfs:\n",
    "            if isinstance(df, DataFrame):\n",
    "                if df.storageLevel != StorageLevel.NONE:\n",
    "                    df.unpersist()\n",
    "                df.cache()\n",
    "            print(f\"Dataset cached. {df.count()} rows.\")\n",
    "    \n",
    "    def parquet_all(self, preserve_partitions=True):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            print(f\"Saving '{name}' as Parquet...\")\n",
    "            if preserve_partitions:\n",
    "                df.write.mode(\"overwrite\").format(\"geoparquet\").save(f\"./pickle_parquets/dfs_list/{name}\")\n",
    "            else:\n",
    "                df.coalesce(1).write.mode(\"overwrite\").format(\"geoparquet\").save(f\"./pickle_parquets/dfs_list/{name}\")\n",
    "            print(f\"Saved to './pickle_parquets/dfs_list/{name}'\")\n",
    "\n",
    "    def parquet_this(self, name, df, preserve_partitions=True):\n",
    "        print(f\"Saving dataframe '{name}' as Parquet...\")\n",
    "        if preserve_partitions:\n",
    "            df.write.mode(\"overwrite\").format(\"geoparquet\").save(f\"./pickle_parquets/others/{name}\")\n",
    "        else:\n",
    "            df.coalesce(1).write.mode(\"overwrite\").format(\"geoparquet\").save(f\"./pickle_parquets/others/{name}\")\n",
    "        print(f\"Saved to './pickle_parquets/others/{name}'\")\n",
    "\n",
    "    def load_from_parquets(self, datasets):\n",
    "        self.datasets = {}\n",
    "        for name, (path, fformat) in datasets.items():\n",
    "            self.datasets[name] = (path, fformat)\n",
    "        \n",
    "        for name, path in self.datasets.items():\n",
    "            print(f\"Loading '{name}' from Parquet...\")\n",
    "            self.dfs_list[name] = self.sedona.read.parquet(f\"./pickle_parquets/dfs_list/{name}\")\n",
    "            print(f\"Loaded dataframe '{name}'\")\n",
    "\n",
    "    def load_from_parquet(self, name):\n",
    "        path = f\"./pickle_parquets/others/{name}\"\n",
    "        self.datasets[name] = (path, \"geoparquet\")\n",
    "        print(f\"Loading dataframe '{name}' from Parquet...\")\n",
    "        df = self.sedona.read.parquet(path)\n",
    "        self.dfs_list[name] = df\n",
    "        print(f\"Loaded dataframe '{name}'\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def pickle_this(self, df, name, preserve_partitions=True):\n",
    "        print(f\"Pickling '{name}'...\")\n",
    "        if preserve_partitions:\n",
    "            df.rdd.saveAsPickleFile(f\"/pickles/{name}\")\n",
    "        else:\n",
    "            df.coalesce(1).rdd.saveAsPickleFile(f\"/pickles/{name}\")\n",
    "        print(f\"Saved to './pickles/{name}'\")\n",
    "\n",
    "    def pickle_all(self, preserve_partitions=True):\n",
    "        for name, df in self.dfs_list.items():\n",
    "            path = f\"./pickles/dfs_list/{name}\"\n",
    "            \n",
    "            print(f\"Pickling '{name}'...\")\n",
    "            schema_json = df.schema.json()\n",
    "            with open(f\"{path}_schema.json\", \"w\") as f:\n",
    "                f.write(schema_json)\n",
    "            if preserve_partitions:\n",
    "                df.rdd.saveAsPickleFile(path)\n",
    "            else:\n",
    "                df.coalesce(1).rdd.saveAsPickleFile(path)\n",
    "            print(f\"Saved to {path}\")\n",
    "        \n",
    "    def load_from_pickle(self, name):\n",
    "        pickleRdd = self.sedona.sparkContext.pickleFile(f\"./pickles/{name}\")\n",
    "        return self.sedona.createDataFrame(pickleRdd)\n",
    "    \n",
    "    def load_from_pickles(self, datasets):\n",
    "        self.datasets = {}\n",
    "        for name, (path, fformat) in datasets.items():\n",
    "            self.datasets[name] = (path, fformat)\n",
    "        \n",
    "        for name, _ in self.datasets.items():\n",
    "            with open(f\"./pickles/dfs_list/{name}_schema.json\", \"r\") as f:\n",
    "                schema_json = f.read()\n",
    "            schema = StructType.fromJson(json.loads(schema_json))\n",
    "            pickleRdd = self.sedona.sparkContext.pickleFile(f\"./pickles/dfs_list/{name}\")\n",
    "            self.dfs_list[name] = self.sedona.createDataFrame(pickleRdd)\n",
    "        \n",
    "    def clear_memory(self, *keep):\n",
    "        if self.res.storageLevel != StorageLevel.NONE:\n",
    "            self.res.unpersist()\n",
    "        self.res = None\n",
    "\n",
    "    def join_chey_simple(self, selected_aggs, df1_name, df2_name):\n",
    "        self.res_agr = self.dfs_list[df1_name].alias(\"df1\").join(\n",
    "            self.dfs_list[df2_name].alias(\"df2\"), F.expr(\"ST_Intersects(df1.geometry, df2.geometry)\")\n",
    "        ).select(\n",
    "            F.expr(\"df1.geometry\").alias(\"df1_geom\"),\n",
    "            F.expr(\"df2.geometry\").alias(\"df2_geom\"),\n",
    "            *[f\"df1.{c}\" for c in self.dfs_list[df1_name].columns if c != \"geometry\"],\n",
    "            *[f\"df2.{c}\" for c in self.dfs_list[df2_name].columns if c != \"geometry\" and c not in self.dfs_list[df1_name].columns]\n",
    "        )\n",
    "    \n",
    "    def join_chey_new(self, selected_aggs, df1_name, df2_name, group_by=None, pred=\"ST_Intersects\", rel_str=\"2********\", make_geom=True, ratio=True, madre=False, cache=True, grid_area=None):\n",
    "        \n",
    "        # temp_df1 = self.dfs_list[df1_name]\n",
    "        # temp_df2 = self.dfs_list[df2_name]\n",
    "    \n",
    "        join_expr = f\"{pred}(df1.geometry, df2.geometry)\"\n",
    "        if pred == \"ST_Relate\":\n",
    "            join_expr = f\"{pred}(df1.geometry, df2.geometry, '{rel_str}')\"\n",
    "    \n",
    "        self.res = self.dfs_list[df1_name].alias(\"df1\").join(\n",
    "            self.dfs_list[df2_name].alias(\"df2\"), F.expr(join_expr)\n",
    "        ).select(\n",
    "            F.expr(\"df1.geometry\").alias(\"df1_geom\"),\n",
    "            F.expr(\"df2.geometry\").alias(\"df2_geom\"),\n",
    "            *[f\"df1.{c}\" for c in self.dfs_list[df1_name].columns if c != \"geometry\"],\n",
    "            *[f\"df2.{c}\" for c in self.dfs_list[df2_name].columns if c != \"geometry\" and c not in self.dfs_list[df1_name].columns]\n",
    "        )\n",
    "        \n",
    "        self.res = self.res.withColumn(\"intr_geometry\", F.expr(\"ST_Intersection(df1_geom, df2_geom)\"))\n",
    "        if ratio:\n",
    "            if grid_area > 0:\n",
    "                self.res = self.res.withColumn(\"intr_ratio\", F.expr(f\"ST_Area(intr_geometry) / {grid_area}\"))\n",
    "            else:\n",
    "                self.res = self.res.withColumn(\"intr_ratio\", F.expr(\"ST_Area(intr_geometry) / ST_Area(df2_geom)\"))\n",
    "\n",
    "            agg_exprs = []\n",
    "            for col_name, agg_func in selected_aggs.items():\n",
    "                if agg_func == \"sum\":\n",
    "                    agg_exprs.append(F.sum(F.col(col_name) * F.col(\"intr_ratio\")).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"mean\":\n",
    "                    agg_exprs.append(F.mean(F.col(col_name) * F.col(\"intr_ratio\")).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"min\":\n",
    "                    agg_exprs.append(F.min(F.col(col_name) * F.col(\"intr_ratio\")).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"max\":\n",
    "                    agg_exprs.append(F.max(F.col(col_name) * F.col(\"intr_ratio\")).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"count\":\n",
    "                    agg_exprs.append(F.count(F.col(col_name)).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                elif agg_func == \"first\":\n",
    "                    agg_exprs.append(F.first(F.col(col_name)).alias(f\"{col_name}_agr_{agg_func}\"))\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported aggregation function: {agg_func}\")\n",
    "                \n",
    "            df1_cols = [F.first(F.col(f\"df1.{c}\")).alias(c) for c in self.dfs_list[df1_name].columns if c != group_by and c != \"geometry\"]\n",
    "            df1_cols.append(F.first(F.col(\"df1_geom\")).alias(\"geometry\"))\n",
    "            self.res_agr = self.res.groupBy(group_by).agg(*df1_cols, *agg_exprs)\n",
    "\n",
    "            print(f\"Aggregation will result in {self.res_agr.count()} rows.\")\n",
    "        \n",
    "        if madre:\n",
    "            print(\"Preserving overlapping geometries to 'res'\")\n",
    "            columns_to_drop = [\"df1_geom\", \"df2_geom\"] + list(selected_aggs.keys())\n",
    "            self.res = self.res.drop(*columns_to_drop)\n",
    "            self.res = self.res.join(self.res_agr.drop(\"geometry\"), on=group_by, how=\"left\")\n",
    "            self.res = self.res.withColumnRenamed(\"intr_geometry\", \"geometry\")\n",
    "            \n",
    "            def drop_duplicate_columns(df):\n",
    "                unique_cols = []\n",
    "                seen = set()\n",
    "                for idx, name in enumerate(df.columns):\n",
    "                    if name.lower() not in seen:\n",
    "                        unique_cols.append((idx, name))\n",
    "                        seen.add(name.lower())\n",
    "\n",
    "                temp_names = [f\"{i}\" for i in range(len(df.columns))]\n",
    "                df_temp = df.toDF(*temp_names)\n",
    "\n",
    "                select_expr = [F.col(f\"{idx}\").alias(name) for idx, name in unique_cols]\n",
    "                return df_temp.select(*select_expr)\n",
    "\n",
    "            self.res = drop_duplicate_columns(self.res)\n",
    "    \n",
    "        if cache:\n",
    "            if madre:\n",
    "                self._make_cache([self.res, self.res_agr])\n",
    "            else:\n",
    "                self._make_cache(self.res_agr)\n",
    "            \n",
    "        return self.res_agr\n",
    "        \n",
    "\n",
    "    def export(self, df=\"default\", path=\"outputs\", name=\"unnamed\", how=\"repartition\", num=None, clear=False):\n",
    "        if num is None:\n",
    "            num = self.cores\n",
    "        if how == \"repartition\":\n",
    "            self.res = self.res.repartition(num)\n",
    "        elif how == \"coalesce\":\n",
    "            self.res = self.res.coalesce(num)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid 'how'. Choose either 'repartition' or 'coalesce'\")\n",
    "\n",
    "        if df == \"default\":\n",
    "            self.res_agr.write.mode(\"overwrite\").format(\"geoparquet\").save(f\"./{path}/\" + f\"/{name}_agr\")\n",
    "        else:\n",
    "            self.res.write.mode(\"overwrite\").format(\"geoparquet\").save(f\"./{path}/\" + f\"/{name}_madre\")\n",
    "        \n",
    "        if clear:\n",
    "            self.clear_memory()\n",
    "\n",
    "\n",
    "\n",
    "class EnricherUI:\n",
    "    def __init__(self, enricher):\n",
    "        self.enricher = enricher\n",
    "        self.loaded_dataframes = {}\n",
    "        self.selected_cols = []\n",
    "        self.group_by_col = None\n",
    "        self._init_ui()\n",
    "        self.loaded_dataframes = self.list_dataframes_in_memory()\n",
    "        self.df1_dropdown.options = list(self.loaded_dataframes.keys())\n",
    "        self.df2_dropdown.options = list(self.loaded_dataframes.keys())\n",
    "        self.df1_dropdown.disabled = False\n",
    "        self.df2_dropdown.disabled = False        \n",
    "        self.selected_aggs = {}\n",
    "        self.agg_options = [\"sum\", \"count\", \"mean\", \"min\", \"max\", \"first\"]\n",
    "\n",
    "    def list_dataframes_in_memory(self):\n",
    "        # return {name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)}\n",
    "        return {name: df for name, df in self.enricher.dfs_list.items()}\n",
    "    \n",
    "    def _init_ui(self):\n",
    "        # Heading:\n",
    "        self.heading = widgets.HTML(value=\"<h1>Enrich with Overlay & Aggregation</h1>\")\n",
    "\n",
    "        # First line: Enrich <df1> with <df2>\n",
    "        self.df1_dropdown = widgets.Dropdown(options=[], description=\"df1:\", disabled=True, style={'description_width': 'initial'}, layout=widgets.Layout(margin=\"5px 20px\", width=\"150px\"))\n",
    "        self.df2_dropdown = widgets.Dropdown(options=[], description=\"df2:\", disabled=True, style={'description_width': 'initial'}, layout=widgets.Layout(margin=\"5px 20px\", width=\"150px\"))\n",
    "        self.load_status = widgets.HTML(value=\"<small>Status: No dataframes loaded.</small>\")\n",
    "        self.load_button = widgets.Button(description=\"Load\", disabled=True, layout=widgets.Layout(margin=\"5px 0px\", width=\"100px\", border=\"2px solid black\"))\n",
    "        self.load_button.style.font_weight = 'bold'\n",
    "\n",
    "        # Second line: with attributes: <cols>\n",
    "        self.cols_dropdown = widgets.SelectMultiple(options=[], description=\"aggr cols:\", disabled=True, style={'description_width': 'initial'})\n",
    "        self.agg_table_output = widgets.Output()        \n",
    "        \n",
    "        # Third line: unique id: <col>\n",
    "        self.group_by_dropdown = widgets.Dropdown(options=[], description=\"unique id:\", disabled=True, style={'description_width': 'initial'}, layout=widgets.Layout(margin=\"5px 20px\", width=\"250px\"))\n",
    "        \n",
    "        self.agg_status = widgets.HTML(value=\"\")\n",
    "\n",
    "        # Advanced options\n",
    "        self.advanced_checkbox = widgets.Checkbox(value=False, description=\"Advanced options\", style={'description_width': 'initial'})\n",
    "        self.preserve_geoms_checkbox = widgets.Checkbox(value=False, description=\"Preserve overlapping geometries\", disabled=False)\n",
    "        self.intersection_ratio_checkbox = widgets.Checkbox(value=False, description=\"Enricher has uniform grids\", disabled=False)\n",
    "        self.grid_area_text = widgets.FloatText(value=1e6, description=\"Grid area:\", disabled=True, layout=widgets.Layout(width=\"165px\"))\n",
    "        self.custom_predicate_checkbox = widgets.Checkbox(value=False, description=\"Custom ST_Relate predicate string:\", disabled=False, layout=Layout(width=\"500px\"))\n",
    "        self.custom_predicate_text = widgets.Text(value=\"2********\", disabled=True, layout=widgets.Layout(width=\"200px\"))\n",
    "        \n",
    "        self.go_button = widgets.Button(description=\"Go\", disabled=True, layout=widgets.Layout(margin=\"5px 0px\", width=\"100px\", border=\"2px solid black\"))\n",
    "        self.go_button.style.font_weight = 'bold'\n",
    "        \n",
    "        # Console output text box\n",
    "        self.console_output = widgets.Output(layout=widgets.Layout(width=\"100%\", height=\"200px\", border=\"1px solid black\"))\n",
    "        self.clear_console_button = widgets.Button(description=\"Clear\", layout=widgets.Layout(width='80px', border=\"2px solid black\"))\n",
    "        self.clear_console_button.style.font_weight = 'bold'\n",
    "\n",
    "        # Layout\n",
    "        self._setup_layout()\n",
    "        self._setup_event_handlers()\n",
    "\n",
    "    def _setup_layout(self):\n",
    "        # First line: Enrich <df1> with <df2>\n",
    "        df_selection_line = widgets.HBox([\n",
    "            widgets.HTML(value=\"<h2 style='display: inline; margin-right: 10px;'>Enrich </h2>\"),\n",
    "            self.df1_dropdown,\n",
    "            widgets.HTML(value=\"<h2 style='display: inline; margin-right: 10px;'> with </h2>\"),\n",
    "            self.df2_dropdown,\n",
    "            self.load_button\n",
    "        ])\n",
    "\n",
    "        # Second line: with attributes: <cols>\n",
    "        cols_selection_line = widgets.HBox([\n",
    "            widgets.HTML(value=\"<h2 style='display: inline; margin-right: 10px;'> with attributes: </h2>\"),\n",
    "            self.cols_dropdown,\n",
    "            self.agg_table_output,\n",
    "        ])\n",
    "\n",
    "        # Third line: unique id: <col>\n",
    "        grp_by_selection_line = widgets.HBox([\n",
    "            widgets.HTML(value=f\"<h2 style='display: inline; margin-right: 10px;'><span id='unique_id_text'>unique identifier:</span> </h2>\"),\n",
    "            self.group_by_dropdown,\n",
    "            self.go_button\n",
    "        ])\n",
    "\n",
    "        # Advanced options\n",
    "        self.advanced_options = widgets.VBox(\n",
    "            [\n",
    "                self.preserve_geoms_checkbox,\n",
    "                widgets.HBox([self.intersection_ratio_checkbox, self.grid_area_text]),\n",
    "                widgets.HBox([self.custom_predicate_checkbox, self.custom_predicate_text]),\n",
    "            ],\n",
    "            layout=widgets.Layout(max_width=\"600px\", display=\"none\")\n",
    "        )\n",
    "\n",
    "        # Main layout\n",
    "        self.main_layout = widgets.VBox([\n",
    "            self.heading,\n",
    "            widgets.HTML(value=\"<div style='height: 5px;'></div>\"),\n",
    "            df_selection_line,\n",
    "            self.load_status,\n",
    "            cols_selection_line,\n",
    "            widgets.HTML(value=\"<div style='height: 4px;'></div>\"),\n",
    "            grp_by_selection_line,\n",
    "            self.agg_status,\n",
    "            widgets.HTML(value=\"<div style='height: 3px;'></div>\"),\n",
    "            self.advanced_checkbox,\n",
    "            self.advanced_options,\n",
    "            widgets.HBox([self.console_output, self.clear_console_button])\n",
    "        ])\n",
    "\n",
    "        # Display everything\n",
    "        display(self.main_layout)\n",
    "\n",
    "    def _setup_event_handlers(self):\n",
    "        # Enable/disable load button based on dataframe selection\n",
    "        def on_df_selection_change(change):\n",
    "            if self.df1_dropdown.value and self.df2_dropdown.value:\n",
    "                self.load_button.disabled = False\n",
    "            else:\n",
    "                self.load_button.disabled = True\n",
    "        self.df1_dropdown.observe(on_df_selection_change, names='value')\n",
    "        self.df2_dropdown.observe(on_df_selection_change, names='value')\n",
    "\n",
    "        # Handle load button click\n",
    "        def on_load_button_click(b):\n",
    "            try:\n",
    "                df1_name = self.df1_dropdown.value\n",
    "                df2_name = self.df2_dropdown.value\n",
    "\n",
    "                if df1_name not in self.loaded_dataframes or df2_name not in self.loaded_dataframes:\n",
    "                    raise ValueError(\"Selected dataframes are not loaded in memory.\")\n",
    "\n",
    "                # Set the selected dataframes in the Enricher\n",
    "                self.enricher.df1 = self.loaded_dataframes[df1_name]\n",
    "                self.enricher.df2 = self.loaded_dataframes[df2_name]\n",
    "\n",
    "                # Update column dropdowns\n",
    "                self.cols_dropdown.options = self.loaded_dataframes[self.df2_dropdown.value].columns\n",
    "                self.group_by_dropdown.options = self.loaded_dataframes[self.df1_dropdown.value].columns\n",
    "                self.cols_dropdown.disabled = False\n",
    "                self.group_by_dropdown.disabled = False\n",
    "\n",
    "                self.load_status.value = f\"<small>Status: Loaded {df1_name} and {df2_name}.</small>\"\n",
    "                self.main_layout.children[6].children[0].value = f\"<h2 style='display: inline; margin-right: 10px;'>{df1_name}'s unique identifier: </h2>\"\n",
    "            except Exception as e:\n",
    "                self.load_status.value = f\"<small>Error: {str(e)}</small>\"\n",
    "\n",
    "        self.load_button.on_click(on_load_button_click)\n",
    "\n",
    "        # Handle column selection\n",
    "        def on_cols_change(change):\n",
    "            for col in change[\"new\"]:\n",
    "                if col not in self.selected_cols:\n",
    "                    self.selected_cols.append(col)\n",
    "            \n",
    "            self.cols_dropdown.options = [col for col in self.loaded_dataframes[self.df2_dropdown.value].columns if col not in self.selected_cols]\n",
    "            \n",
    "            # Preserve previously selected operations, default to \"sum\" for new columns\n",
    "            for col in self.selected_cols:\n",
    "                if col not in self.selected_aggs:\n",
    "                    self.selected_aggs[col] = \"sum\"            \n",
    "            \n",
    "            def generate_agg_table():\n",
    "                headers = [\"Selected Column\", \"Operation\", \"\"]\n",
    "                \n",
    "                cell_style = widgets.Layout(\n",
    "                    # border=\"1px solid black\", \n",
    "                    padding=\"0px 2px\",\n",
    "                    align_items=\"center\", \n",
    "                    justify_content=\"center\", \n",
    "                    width=\"125px\"\n",
    "                )\n",
    "                \n",
    "                clr_style = widgets.Layout(border=\"1px solid black\", padding=\"0px 2px\",align_items=\"center\", justify_content=\"center\", width=\"80px\")\n",
    "                \n",
    "                header_row = [\n",
    "                    widgets.HTML(f\"<b>{headers[0]}</b>\", layout=cell_style),\n",
    "                    widgets.HTML(f\"<b>{headers[1]}</b>\", layout=cell_style),\n",
    "                    widgets.HTML(\"\", layout=widgets.Layout(padding=\"0px 2px\",align_items=\"center\", justify_content=\"center\", width=\"80px\"))\n",
    "                ]\n",
    "            \n",
    "                rows = []\n",
    "                for col in self.selected_aggs:\n",
    "                    dropdown = widgets.Dropdown(\n",
    "                        options=self.agg_options, \n",
    "                        value=self.selected_aggs[col], \n",
    "                        layout=cell_style\n",
    "                    )\n",
    "                    dropdown.observe(lambda change, col=col: self.selected_aggs.update({col: change[\"new\"]}), names=\"value\")\n",
    "            \n",
    "                    clear_button = widgets.Button(description=\"Clear\", layout=clr_style)\n",
    "\n",
    "                    def on_clear(btn, col=col):\n",
    "                        self.selected_cols.remove(col)\n",
    "                        self.agg_status.value = f\"Status: Aggregating with: {', '.join([f'<b>{col}</b>' for col in self.selected_cols]) if self.selected_cols else '<i>select cols</i>'}, grouping by: {f'<b>{self.group_by_col}</b>' if self.group_by_col else '<i>select cols</i>'}\"\n",
    "                        del self.selected_aggs[col]\n",
    "                        self.cols_dropdown.options = [col for col in self.loaded_dataframes[self.df2_dropdown.value].columns if col not in self.selected_cols]\n",
    "                        \n",
    "                        with self.agg_table_output:\n",
    "                            self.agg_table_output.clear_output()\n",
    "                            display(generate_agg_table())\n",
    "                        \n",
    "                    clear_button.on_click(on_clear)\n",
    "                        \n",
    "                    rows.extend([\n",
    "                        widgets.HTML(col, layout=cell_style),\n",
    "                        dropdown,\n",
    "                        clear_button\n",
    "                    ])\n",
    "                \n",
    "                scrollable_container = widgets.VBox([\n",
    "                    widgets.GridBox(\n",
    "                        children=header_row + rows,\n",
    "                        layout=widgets.Layout(\n",
    "                            grid_template_columns=\"150px 150px 90px\",\n",
    "                            grid_template_rows=\"auto\",\n",
    "                            padding=\"1px\",\n",
    "                            width=\"max-content\",\n",
    "                        )\n",
    "                    )\n",
    "                ], layout=widgets.Layout(\n",
    "                    max_height=\"150px\",\n",
    "                    overflow_y=\"auto\",\n",
    "                    border=\"1px solid black\"\n",
    "                ))\n",
    "            \n",
    "                return scrollable_container\n",
    "\n",
    "\n",
    "            with self.agg_table_output:\n",
    "                self.agg_table_output.clear_output()\n",
    "                display(generate_agg_table())\n",
    "\n",
    "            self.agg_status.value = f\"Status: Will aggregate with: {', '.join([f'<b>{col}</b>' for col in self.selected_cols]) if self.selected_cols else '<i>select cols</i>'}; grouping by: {f'<b>{self.group_by_col}</b>' if self.group_by_col else '<i>select cols</i>'}\"\n",
    "            if self.group_by_col and self.selected_cols:\n",
    "                self.go_button.disabled = False            \n",
    "        self.cols_dropdown.observe(on_cols_change, names='value')\n",
    "\n",
    "        \n",
    "        def on_group_by_change(change):\n",
    "            self.group_by_col = change[\"new\"]\n",
    "            self.agg_status.value = f\"Status: Will aggregate with: {', '.join([f'<b>{col}</b>' for col in self.selected_cols]) if self.selected_cols else '<i>select col</i>'}; grouping by: {f'<b>{self.group_by_col}</b>' if self.group_by_col else '<i>select col</i>'}\"\n",
    "            if self.group_by_col and self.selected_cols:\n",
    "                self.go_button.disabled = False                \n",
    "                \n",
    "\n",
    "        self.group_by_dropdown.observe(on_group_by_change, names='value')\n",
    "\n",
    "\n",
    "        def on_advanced_checkbox_change(change):\n",
    "            if change[\"new\"]:\n",
    "                self.advanced_options.layout.display = \"block\"                \n",
    "\n",
    "            else:\n",
    "                self.advanced_options.layout.display = \"none\"\n",
    "\n",
    "        self.advanced_checkbox.observe(on_advanced_checkbox_change, names='value')\n",
    "\n",
    "        def on_intersection_ratio_change(change):\n",
    "            self.grid_area_text.disabled = not change[\"new\"]\n",
    "        self.intersection_ratio_checkbox.observe(on_intersection_ratio_change, names='value')\n",
    "\n",
    "        def on_custom_predicate_change(change):\n",
    "            self.custom_predicate_text.disabled = not change[\"new\"]\n",
    "        self.custom_predicate_checkbox.observe(on_custom_predicate_change, names='value')\n",
    "\n",
    "        def on_go_button_click(b):\n",
    "            with self.console_output:\n",
    "                try:\n",
    "                    print(\"Performing operation. This may take a while. Check logs for Spark logs and completion status.\")\n",
    "                    self.enricher.join_chey_new(\n",
    "                        selected_aggs=self.selected_aggs,\n",
    "                        df1_name=self.df1_dropdown.value,\n",
    "                        df2_name=self.df2_dropdown.value,\n",
    "                        group_by=self.group_by_col,\n",
    "                        pred=\"ST_Relate\" if self.custom_predicate_checkbox.value else \"ST_Intersects\",\n",
    "                        rel_str=self.custom_predicate_text.value if self.custom_predicate_checkbox.value else \"2********\",\n",
    "                        make_geom=True,\n",
    "                        ratio=self.intersection_ratio_checkbox.value,\n",
    "                        madre=self.preserve_geoms_checkbox.value,\n",
    "                        cache=True,\n",
    "                        grid_area=float(self.grid_area_text.value) if self.intersection_ratio_checkbox.value else 1e6\n",
    "                    )\n",
    "                    print(\"Enrichment operation completed.\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {str(e)}\")\n",
    "        \n",
    "        self.go_button.on_click(on_go_button_click)\n",
    "\n",
    "        def on_clear_console_button_click(b):\n",
    "            self.console_output.clear_output()\n",
    "        self.clear_console_button.on_click(on_clear_console_button_click)\n",
    "\n",
    "    def add_dataframe(self, name, dataframe):\n",
    "        self.loaded_dataframes[name] = dataframe\n",
    "        self.df1_dropdown.options = list(self.loaded_dataframes.keys())\n",
    "        self.df2_dropdown.options = list(self.loaded_dataframes.keys())\n",
    "        self.df1_dropdown.disabled = False\n",
    "        self.df2_dropdown.disabled = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# file paths:\n",
    "\n",
    "path_contr = \"./data_EU/countries_shp/\"\n",
    "path_reg = \"./data_Italy/regioni/\"\n",
    "path_prov = \"./data_Italy/provinci\"\n",
    "path_com_EU = \"./data_EU/comuni_shp/\"\n",
    "path_com = \"./data_Italy/comuni/\"\n",
    "path_grids = \"./data_EU/census_grid_EU/grids_corrected.parquet\"\n",
    "path_grids_new = \"./data_EU/census_grid_EU/grids_new.gpkg\"\n",
    "\n",
    "\n",
    "# datasets:\n",
    "# format: {display_name: (path, file_format), ...}\n",
    "\n",
    "datasets = {\n",
    "    \"countries\": (path_contr, \"shapefile\"),\n",
    "    \"regions_IT\": (path_reg, \"shapefile\"),\n",
    "    \"provinces_IT\": (path_prov, \"shapefile\"),\n",
    "    \"comuni_EU\": (path_com_EU, \"shapefile\"),\n",
    "    \"comuni_IT\": (path_com, \"shapefile\"),\n",
    "    \"pop_grids\": (path_grids, \"geoparquet\"),\n",
    "    \"pop_grids_new\": (path_grids_new, \"geopackage\")\n",
    "    # \"census\": (path_census, \"\"),\n",
    "}\n",
    "\n",
    "\n",
    "obj = Enricher(crs=\"EPSG:3035\")\n",
    "obj.setup_cluster(which=\"sedona\", ex_mem=26, dr_mem=24, log_level=\"ERROR\")\n",
    "\n",
    "obj.load(datasets, silent=True)\n",
    "obj.fix_geometries(skip=['pop_grids', 'pop_grids_new'])\n",
    "obj.force_repartition(skip=['pop_grids'])\n",
    "# obj.inspect_partitions()\n",
    "obj.transform(lazy=False)\n",
    "obj.parquet_all(preserve_partitions=True)\n",
    "\n",
    "\n",
    "# obj.dfs_list['comuni_EU'] = obj.dfs_list['comuni_EU'].filter(F.col('CNTR_ID').isin([\"IT\", \"DE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedona initialized with 10 cores for parellelism.\n",
      "Loading 'countries' from Parquet...\n",
      "Loaded dataframe 'countries'\n",
      "Loading 'regions_IT' from Parquet...\n",
      "Loaded dataframe 'regions_IT'\n",
      "Loading 'provinces_IT' from Parquet...\n",
      "Loaded dataframe 'provinces_IT'\n",
      "Loading 'comuni_EU' from Parquet...\n",
      "Loaded dataframe 'comuni_EU'\n",
      "Loading 'comuni_IT' from Parquet...\n",
      "Loaded dataframe 'comuni_IT'\n",
      "Loading 'pop_grids' from Parquet...\n",
      "Loaded dataframe 'pop_grids'\n",
      "Loading 'pop_grids_new' from Parquet...\n",
      "Loaded dataframe 'pop_grids_new'\n"
     ]
    }
   ],
   "source": [
    "# Load from pickles\n",
    "\n",
    "path_contr = \"./data_EU/countries_shp/\"\n",
    "path_reg = \"./data_Italy/regioni/\"\n",
    "path_prov = \"./data_Italy/provinci\"\n",
    "path_com_EU = \"./data_EU/comuni_shp/\"\n",
    "path_com = \"./data_Italy/comuni/\"\n",
    "path_grids = \"./data_EU/census_grid_EU/grids_corrected.parquet\"\n",
    "path_grids_new = \"./data_EU/census_grid_EU/grids_new.gpkg\"\n",
    "\n",
    "\n",
    "# datasets:\n",
    "# format: {display_name: (path, file_format), ...}\n",
    "\n",
    "datasets = {\n",
    "    \"countries\": (path_contr, \"shapefile\"),\n",
    "    \"regions_IT\": (path_reg, \"shapefile\"),\n",
    "    \"provinces_IT\": (path_prov, \"shapefile\"),\n",
    "    \"comuni_EU\": (path_com_EU, \"shapefile\"),\n",
    "    \"comuni_IT\": (path_com, \"shapefile\"),\n",
    "    \"pop_grids\": (path_grids, \"geoparquet\"),\n",
    "    \"pop_grids_new\": (path_grids_new, \"geopackage\")\n",
    "    # \"census\": (path_census, \"\"),\n",
    "}\n",
    "\n",
    "obj = Enricher(crs=\"EPSG:3035\")\n",
    "obj.setup_cluster(which=\"sedona\", ex_mem=26, dr_mem=24, log_level=\"ERROR\")\n",
    "obj.load_from_parquets(datasets)\n",
    "# obj.inspect_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMM_ID', 'CNTR_ID', 'CNTR_CODE', 'COMM_NAME', 'NAME_ASCI', 'TRUE_FLAG', 'NSI_CODE', 'NAME_NSI', 'NAME_LATN', 'NUTS_CODE', 'FID', 'DIST_BORD', 'TOT_P_2018', 'TOT_P_2006', 'GRD_ID', 'TOT_P_2011', 'Y_LLC', 'NUTS2016_3', 'NUTS2016_2', 'NUTS2016_1', 'NUTS2016_0', 'LAND_PC', 'X_LLC', 'NUTS2021_3', 'NUTS2021_2', 'NUTS2021_1', 'NUTS2021_0', 'geometry', 'intr_ratio', 'TOT_P_2021_agr_sum', 'DIST_COAST_agr_mean']\n",
      "Saving dataframe 'pop_new_X_comuni_EU_res' as Parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to './pickle_parquets/others/pop_new_X_comuni_EU_res'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# obj.parquet_this(\"pop_new_X_comuni_EU\", obj.res_agr, preserve_partitions=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obj.parquet_this(\"pop_new_X_comuni_EU_res\", obj.res, preserve_partitions=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e332c2042c3d42dcb5105e7cd68e6029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Enrich with Overlay & Aggregation</h1>'), HTML(value=\"<div style='height: 5px;'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GUI\n",
    "\n",
    "obj_ui = EnricherUI(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unique_values = obj.res.select('CNTR_ID').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e98c5211a54f269361bdb28013d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'pop_grids': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import decimal\n",
    "\n",
    "temp_df = obj.res_agr\n",
    "# temp_df = obj.res.filter(F.col('CNTR_ID').isin(\"NL\", \"BE\", \"DE\", \"IT\"))\n",
    "# temp_df = obj.dfs_list['comuni_EU']\n",
    "# temp_df = obj.dfs_list['pop_grids'].filter(F.col('T')>1000)\n",
    "\n",
    "\n",
    "def prep_for_map(res_agr, crs, geom_col='geometry'):\n",
    "    df = res_agr.toPandas()\n",
    "    df = df.map(lambda x: float(x) if isinstance(x, decimal.Decimal) else x)\n",
    "    df['geometry'] = df[f'{geom_col}'].apply(lambda geom: shape(geom))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    gdf.crs = crs\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=prep_for_map(temp_df, obj.crs, geom_col=\"geometry\"), name=\"pop_grids\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import decimal\n",
    "\n",
    "# temp_df = obj.res_agr\n",
    "temp_df = obj.dfs_list['pop_grids_new']\n",
    "# temp_df = obj.dfs_list['comuni_EU']\n",
    "# temp_df = obj.dfs_list['pop_grids_new'].filter(F.col('CNTR_ID').isin(\"NL\", \"BE\", \"DE\", \"IT\"))\n",
    "\n",
    "\n",
    "def prep_for_map(res_agr, crs, geom_col='geometry'):\n",
    "    df = res_agr.toPandas()\n",
    "    df = df.map(lambda x: float(x) if isinstance(x, decimal.Decimal) else x)\n",
    "    df['geometry'] = df[f'{geom_col}'].apply(lambda geom: shape(geom))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "    gdf.crs = crs\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=prep_for_map(temp_df, obj.crs, geom_col=\"geometry\"), name=\"pop_grids\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_EU_com_enriched = obj.res_agr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a537a4fb6604e8fab471ca6fb390dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'res_agr': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "com_pop = obj.res_agr\n",
    "\n",
    "res_agr = com_pop.toPandas()\n",
    "res_agr['geometry'] = res_agr['geometry'].apply(lambda geom: shape(geom))\n",
    "\n",
    "gdf = gpd.GeoDataFrame(res_agr, geometry='geometry')\n",
    "gdf.crs = \"EPSG:3035\"\n",
    "\n",
    "map = KeplerGl(height=600)\n",
    "map.add_data(data=gdf, name=\"res_agr\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sedona_venv)",
   "language": "python",
   "name": "sedona_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
